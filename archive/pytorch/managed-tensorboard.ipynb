{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitoring PyTorch experiments with Managed TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import google.auth\n",
    "\n",
    "from google.auth.credentials import Credentials\n",
    "from google.auth.transport.requests import AuthorizedSession\n",
    "\n",
    "from typing import List, Optional, Text, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAIP_ENDPOINT = \"us-central1-aiplatform.googleapis.com\"\n",
    "CAIP_REGION = \"us-central1\"\n",
    "TENSORBOARD_NAME = \"pytorch-tensorboard\"\n",
    "GCS_BUCKET = \"gs://jk-mlops-dev-tensorboard-logs-us-central1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Managed TensorBoard instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an authorized session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials, project_id = google.auth.default()\n",
    "authed_session = AuthorizedSession(credentials)\n",
    "\n",
    "alpha_api_prefix = f'https://{CAIP_ENDPOINT}/v1alpha1/projects/{project_id}/locations/{CAIP_REGION}'\n",
    "beta_api_prefix = f'https://{CAIP_ENDPOINT}/v1beta1/projects/{project_id}/locations/{CAIP_REGION}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a TensorBoard resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_url = f'{alpha_api_prefix}/tensorboards'\n",
    "\n",
    "request_body = {\n",
    "    \"display_name\": TENSORBOARD_NAME\n",
    "}\n",
    "\n",
    "response = authed_session.post(api_url, data=json.dumps(request_body))\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List all tensorboards with a set name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tensorboards': [{'name': 'projects/895222332033/locations/us-central1/tensorboards/1989183660414205952',\n",
       "   'displayName': 'pytorch-tensorboard',\n",
       "   'createTime': '2020-12-01T05:20:26.567702Z',\n",
       "   'updateTime': '2020-12-01T05:20:26.695088Z',\n",
       "   'etag': 'AMEw9yP3o0igiZefrWbetmFqoDaTRvf86o_XtNCH99_JXhlYEqLqVV3lz8UtmA5_uyFo'}]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_url = f'{alpha_api_prefix}/tensorboards?filter=display_name={TENSORBOARD_NAME}'\n",
    "\n",
    "response = authed_session.get(api_url)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing a training container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train_eval.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train_eval.py\n",
    "\n",
    "# Copyright 2020 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#            http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "\n",
    "import argparse\n",
    "import hypertune\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "\n",
    "DEFAULT_ROOT = '/tmp'\n",
    "\n",
    "def get_catsanddogs(root):\n",
    "    \"\"\"\n",
    "    Creates training and validation Datasets based on images\n",
    "    of cats and dogs from \n",
    "    https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Download and extract the images\n",
    "    source_url = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'\n",
    "    local_filename = source_url.split('/')[-1]\n",
    "    datasets.utils.download_url(source_url, root, )\n",
    "    path_to_zip = os.path.join(root, local_filename)\n",
    "    with zipfile.ZipFile(path_to_zip, 'r') as zip_ref:\n",
    "        zip_ref.extractall(root)\n",
    "    \n",
    "    \n",
    "    # Create datasets\n",
    "    train_transforms = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(256),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.CenterCrop(size=224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "    \n",
    "    val_transforms = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "    \n",
    "    train_dataset = datasets.ImageFolder(\n",
    "        root=os.path.join(path_to_zip[:-4], 'train'),\n",
    "        transform=train_transforms)\n",
    "    \n",
    "    val_dataset = datasets.ImageFolder(\n",
    "        root=os.path.join(path_to_zip[:-4], 'validation'),\n",
    "        transform=val_transforms\n",
    "    )\n",
    "    \n",
    "    return train_dataset, val_dataset\n",
    "    \n",
    "    \n",
    "    \n",
    "def get_data(data_dir, batch_size):\n",
    "    \"\"\"Creates training and validation splits.\"\"\"\n",
    "\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.RandomResizedCrop(256),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.CenterCrop(size=224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    }\n",
    "\n",
    "    data = {\n",
    "        'train':\n",
    "            datasets.ImageFolder(root=os.path.join(data_dir, 'train'),\n",
    "                                 transform=data_transforms['train']),\n",
    "        'val':\n",
    "            datasets.ImageFolder(root=os.path.join(data_dir, 'validation'),\n",
    "                                 transform=data_transforms['val'])\n",
    "    }\n",
    "\n",
    "    dataloaders = {\n",
    "        'train':\n",
    "            DataLoader(data['train'], batch_size=batch_size, shuffle=True),\n",
    "        'val':\n",
    "            DataLoader(data['val'], batch_size=batch_size, shuffle=True)\n",
    "    }\n",
    "\n",
    "    class_names = data['train'].classes\n",
    "\n",
    "    return dataloaders, class_names\n",
    "\n",
    "\n",
    "def get_model(num_layers, dropout_ratio, num_classes):\n",
    "    \"\"\"\n",
    "    Creates a convolution net using ResNet50 trunk and\n",
    "    a custom head.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the ResNet50 trunk\n",
    "    model = models.resnet18(pretrained=True)\n",
    "\n",
    "    # Get the number of input features to the default head\n",
    "    num_features = model.fc.in_features\n",
    "\n",
    "    # Freeze trunk weights\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Define the new head\n",
    "    head = nn.Sequential(nn.Linear(num_features, num_layers),\n",
    "                         nn.ReLU(),\n",
    "                         nn.Dropout(dropout_ratio),\n",
    "                         nn.Linear(num_layers, num_classes))\n",
    "\n",
    "    # Replace the head\n",
    "    model.fc = head\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_eval(device, model, train_dataloader, valid_dataloader,\n",
    "               criterion, optimizer, scheduler, num_epochs, writer=None):\n",
    "    \"\"\"\n",
    "    Trains and evaluates a model.\n",
    "    \"\"\"\n",
    "    since = time.time()\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    hpt = hypertune.HyperTune()\n",
    "\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        num_train_examples = 0\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for inputs, labels in train_dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            num_train_examples += inputs.size(0)\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "        scheduler.step()\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        num_val_examples = 0\n",
    "        val_loss = 0\n",
    "        val_corrects = 0\n",
    "\n",
    "        for inputs, labels in valid_dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            num_val_examples += inputs.size(0)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            val_corrects += torch.sum(torch.eq(torch.max(outputs, 1)\n",
    "                                               [1], labels))\n",
    "\n",
    "        # Log epoch metrics\n",
    "        train_loss = train_loss / num_train_examples\n",
    "        val_loss = val_loss / num_val_examples\n",
    "        val_acc = val_corrects.double() / num_val_examples\n",
    "\n",
    "        print('Epoch: {}/{}, Training loss: {:.3f}, Validation loss: {:.3f}, Validation accuracy: {:.3f}'.format(\n",
    "              epoch, num_epochs, train_loss, val_loss, val_acc))\n",
    "\n",
    "        # Write to Tensorboard\n",
    "        if writer:\n",
    "            writer.add_scalars(\n",
    "                'Loss', {'training': train_loss, 'validation': val_loss}, epoch)\n",
    "            writer.add_scalar('Validation accuracy', val_acc, epoch)\n",
    "            writer.flush()\n",
    "            \n",
    "        # Report to HyperTune\n",
    "        hpt.report_hyperparameter_tuning_metric(\n",
    "            hyperparameter_metric_tag='accuracy'\n",
    "            metric_value=val_acc,\n",
    "            global_step=epoch\n",
    "        )\n",
    "\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, best_acc\n",
    "\n",
    "\n",
    "def get_args():\n",
    "    \"\"\"\n",
    "    Returns parsed command line arguments.\n",
    "    \"\"\"\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        '--num-epochs',\n",
    "        type=int,\n",
    "        default=20,\n",
    "        help='number of times to go through the data, default=20')\n",
    "    parser.add_argument(\n",
    "        '--batch-size',\n",
    "        default=32,\n",
    "        type=int,\n",
    "        help='number of records to read during each training step, default=32')\n",
    "    parser.add_argument(\n",
    "        '--num-layers',\n",
    "        default=64,\n",
    "        type=int,\n",
    "        help='number of hidden layers in the classification head , default=64')\n",
    "    parser.add_argument(\n",
    "        '--dropout-ratio',\n",
    "        default=0.5,\n",
    "        type=float,\n",
    "        help='dropout ration in the classification head , default=128')\n",
    "    parser.add_argument(\n",
    "        '--step-size',\n",
    "        default=7,\n",
    "        type=int,\n",
    "        help='step size of LR scheduler')\n",
    "    parser.add_argument(\n",
    "        '--log-dir',\n",
    "        type=str,\n",
    "        default='/tmp',\n",
    "        help='directory for TensorBoard logs')\n",
    "    parser.add_argument(\n",
    "        '--verbosity',\n",
    "        choices=['DEBUG', 'ERROR', 'FATAL', 'INFO', 'WARN'],\n",
    "        default='INFO')\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Parse command line arguments\n",
    "    args = get_args()\n",
    "    \n",
    "    # Create train and validation dataloaders\n",
    "    train_dataset, val_dataset = get_catsanddogs(DEFAULT_ROOT)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "    class_names = train_dataset.classes\n",
    "    \n",
    "    # Use GPU if available\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print('-' * 10)\n",
    "    print(f'Training on device: {device}')\n",
    "\n",
    "    # Configure training\n",
    "    model = get_model(args.num_layers, args.dropout_ratio, len(class_names))\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer, step_size=args.step_size, gamma=0.1)\n",
    "\n",
    "    # Set location for the TensorBoard logs\n",
    "    if 'AIP_TENSORBOARD_LOG_DIR' in os.environ:\n",
    "        log_dir = os.environ['AIP_TENSORBOARD_LOG_DIR']\n",
    "    else:\n",
    "        log_dir = args.log_dir\n",
    "\n",
    "    with SummaryWriter(log_dir) as writer:\n",
    "        trained_model, accuracy = train_eval(device, model, train_dataloader, val_dataloader,\n",
    "                                             criterion, optimizer, scheduler, args.num_epochs, writer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile\n",
    "\n",
    "FROM gcr.io/deeplearning-platform-release/pytorch-gpu.1-6\n",
    "    \n",
    "RUN pip install -U tensorflow cloudml-hypertune\n",
    "\n",
    "ADD train_eval.py .\n",
    "\n",
    "ENTRYPOINT [\"python3\", \"train_eval.py\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_name = 'image_classifier'\n",
    "image_tag = 'latest'\n",
    "image_uri = f'gcr.io/{project_id}/{image_name}:{image_tag}'\n",
    "\n",
    "#!gcloud builds submit --tag {image_uri} ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting a training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = \"JOB_{}\".format(time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "base_output_dir = f'{GCS_BUCKET}/{job_name}'\n",
    "tb_name = 'projects/895222332033/locations/us-central1/tensorboards/1989183660414205952'\n",
    "sa_email = 'aip-training@jk-mlops-dev.iam.gserviceaccount.com'\n",
    "\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'projects/895222332033/locations/us-central1/customJobs/6556132749730643968',\n",
       " 'displayName': 'JOB_20201202_001718',\n",
       " 'jobSpec': {'workerPoolSpecs': [{'machineSpec': {'machineType': 'n1-standard-8',\n",
       "     'acceleratorType': 'NVIDIA_TESLA_T4',\n",
       "     'acceleratorCount': 1},\n",
       "    'replicaCount': '1',\n",
       "    'containerSpec': {'imageUri': 'gcr.io/jk-mlops-dev/image_classifier:latest',\n",
       "     'args': ['--num-epochs=10']}}],\n",
       "  'serviceAccount': 'aip-training@jk-mlops-dev.iam.gserviceaccount.com',\n",
       "  'baseOutputDirectory': {'outputUriPrefix': 'gs://jk-mlops-dev-tensorboard-logs-us-central1/JOB_20201202_001718'},\n",
       "  'tensorboard': 'projects/895222332033/locations/us-central1/tensorboards/1989183660414205952'},\n",
       " 'state': 'JOB_STATE_PENDING',\n",
       " 'createTime': '2020-12-02T00:17:22.361410Z',\n",
       " 'updateTime': '2020-12-02T00:17:22.361410Z'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_url = f'{beta_api_prefix}/customJobs'\n",
    "\n",
    "request_body = {\n",
    "    'display_name': job_name,\n",
    "    'job_spec': {\n",
    "        'worker_pool_specs': [\n",
    "            {\n",
    "                'replica_count': 1,\n",
    "                'machine_spec': {\n",
    "                    'machine_type': 'n1-standard-8',\n",
    "                    'accelerator_type': 'NVIDIA_TESLA_T4',\n",
    "                    'accelerator_count': 1\n",
    "                },\n",
    "                'container_spec': {\n",
    "                    'image_uri': image_uri,\n",
    "                    'args': [\n",
    "                        f'--num-epochs={num_epochs}'\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "        'base_output_directory': {\n",
    "            'output_uri_prefix': base_output_dir,\n",
    "        },\n",
    "        'service_account': sa_email,\n",
    "        'tensorboard': tb_name\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "response = authed_session.post(api_url, data=json.dumps(request_body))\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List all tensorboards in the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_url = f'{alpha_api_prefix}/tensorboards'\n",
    "\n",
    "response = authed_session.get(api_url)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete a TensorBoard resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_id = '7641201192764178432'\n",
    "\n",
    "api_url = f'{alpha_api_prefix}/tensorboards/{tensorboard_id}'\n",
    "\n",
    "response = authed_session.delete(api_url)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parking lot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize a few images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "\n",
    "# Get a batch of training data\n",
    "inputs, classes = next(trainiter)\n",
    "\n",
    "# Make a grid from batch\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "imshow(out, title=[class_names[x] for x in classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(num_layers, dropout_ratio, num_classes):\n",
    "    \"\"\"\n",
    "    Creates a convolution net using ResNet50 trunk and\n",
    "    a custom head.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the number of input features to the default head\n",
    "    model = models.resnet50(pretrained=True)\n",
    "    num_features = model.fc.in_features\n",
    "    \n",
    "    # Freeze trunk weights\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Define a new head\n",
    "    head = nn.Sequential(nn.Linear(num_features, num_layers),\n",
    "                         nn.ReLU(),\n",
    "                         nn.Dropout(dropout_ratio),\n",
    "                         nn.Linear(num_layers, num_classes))\n",
    "    \n",
    "    # Replace the head\n",
    "    model.fc = head\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 64\n",
    "dropout_ratio = 0.5\n",
    "num_classes = 2\n",
    "\n",
    "model = get_model(num_layers, dropout_ratio, num_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'{total_params:,} total parameters.')\n",
    "total_trainable_params = sum(\n",
    "    p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'{total_trainable_params:,} training parameters.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def train_model(device, dataloaders, dataset_sizes, model, criterion, \n",
    "                optimizer, scheduler, num_epochs=25, log_dir='/tmp'):\n",
    "    \n",
    "    since = time.time()\n",
    "    writer = SummaryWriter(log_dir)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    model = model.to(device)\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "            \n",
    "            # Write loss and accuracy to TensorBoard\n",
    "            writer.add_scalar(f'Loss/{phase}', epoch_loss, epoch)\n",
    "            writer.add_scalar(f'Acc/{phase}', epoch_acc, epoch)\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    writer.close()\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "\n",
    "\n",
    "data_dir = '/tmp/data/hymenoptera_data'\n",
    "batch_size = 32\n",
    "log_dir = 'gs://jk-tensorboards/experiments/102'\n",
    "num_epochs = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders, datasizes, class_names = get_dataloaders(data_dir, batch_size)\n",
    "\n",
    "model = train_model(device, dataloaders, datasizes, model, criterion, \n",
    "                    optimizer_ft, exp_lr_scheduler, num_epochs, log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorBoard setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "log_dir = 'gs://jk-tensorboards/experiments/1'\n",
    "\n",
    "writer = SummaryWriter(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# create grid of images\n",
    "img_grid = torchvision.utils.make_grid(images)\n",
    "\n",
    "# show images\n",
    "matplotlib_imshow(img_grid, one_channel=True)\n",
    "\n",
    "# write to tensorboard\n",
    "writer.add_image('four_fashion_mnist_images', img_grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.add_graph(net, images)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_n_random(data, labels, n=100):\n",
    "    '''\n",
    "    Selects n random datapoints and their corresponding labels from a dataset\n",
    "    '''\n",
    "    assert len(data) == len(labels)\n",
    "\n",
    "    perm = torch.randperm(len(data))\n",
    "    return data[perm][:n], labels[perm][:n]\n",
    "\n",
    "# select random images and their target indices\n",
    "images, labels = select_n_random(trainset.data, trainset.targets)\n",
    "\n",
    "# get the class labels for each image\n",
    "class_labels = [classes[lab] for lab in labels]\n",
    "\n",
    "# log embeddings\n",
    "features = images.view(-1, 28 * 28)\n",
    "writer.add_embedding(features,\n",
    "                    metadata=class_labels,\n",
    "                    label_img=images.unsqueeze(1))\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def images_to_probs(net, images):\n",
    "    '''\n",
    "    Generates predictions and corresponding probabilities from a trained\n",
    "    network and a list of images\n",
    "    '''\n",
    "    output = net(images)\n",
    "    # convert output probabilities to predicted class\n",
    "    _, preds_tensor = torch.max(output, 1)\n",
    "    preds = np.squeeze(preds_tensor.numpy())\n",
    "    return preds, [F.softmax(el, dim=0)[i].item() for i, el in zip(preds, output)]\n",
    "\n",
    "\n",
    "def plot_classes_preds(net, images, labels):\n",
    "    '''\n",
    "    Generates matplotlib Figure using a trained network, along with images\n",
    "    and labels from a batch, that shows the network's top prediction along\n",
    "    with its probability, alongside the actual label, coloring this\n",
    "    information based on whether the prediction was correct or not.\n",
    "    Uses the \"images_to_probs\" function.\n",
    "    '''\n",
    "    preds, probs = images_to_probs(net, images)\n",
    "    # plot the images in the batch, along with predicted and true labels\n",
    "    fig = plt.figure(figsize=(12, 48))\n",
    "    for idx in np.arange(4):\n",
    "        ax = fig.add_subplot(1, 4, idx+1, xticks=[], yticks=[])\n",
    "        matplotlib_imshow(images[idx], one_channel=True)\n",
    "        ax.set_title(\"{0}, {1:.1f}%\\n(label: {2})\".format(\n",
    "            classes[preds[idx]],\n",
    "            probs[idx] * 100.0,\n",
    "            classes[labels[idx]]),\n",
    "                    color=(\"green\" if preds[idx]==labels[idx].item() else \"red\"))\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_loss = 0.0\n",
    "for epoch in range(1):  # loop over the dataset multiple times\n",
    "\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:    # every 1000 mini-batches...\n",
    "\n",
    "            # ...log the running loss\n",
    "            writer.add_scalar('training loss',\n",
    "                            running_loss / 1000,\n",
    "                            epoch * len(trainloader) + i)\n",
    "\n",
    "            # ...log a Matplotlib Figure showing the model's predictions on a\n",
    "            # random mini-batch\n",
    "            writer.add_figure('predictions vs. actuals',\n",
    "                            plot_classes_preds(net, inputs, labels),\n",
    "                            global_step=epoch * len(trainloader) + i)\n",
    "            running_loss = 0.0\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. gets the probability predictions in a test_size x num_classes Tensor\n",
    "# 2. gets the preds in a test_size Tensor\n",
    "# takes ~10 seconds to run\n",
    "class_probs = []\n",
    "class_preds = []\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        output = net(images)\n",
    "        class_probs_batch = [F.softmax(el, dim=0) for el in output]\n",
    "        _, class_preds_batch = torch.max(output, 1)\n",
    "\n",
    "        class_probs.append(class_probs_batch)\n",
    "        class_preds.append(class_preds_batch)\n",
    "\n",
    "test_probs = torch.cat([torch.stack(batch) for batch in class_probs])\n",
    "test_preds = torch.cat(class_preds)\n",
    "\n",
    "# helper function\n",
    "def add_pr_curve_tensorboard(class_index, test_probs, test_preds, global_step=0):\n",
    "    '''\n",
    "    Takes in a \"class_index\" from 0 to 9 and plots the corresponding\n",
    "    precision-recall curve\n",
    "    '''\n",
    "    tensorboard_preds = test_preds == class_index\n",
    "    tensorboard_probs = test_probs[:, class_index]\n",
    "\n",
    "    writer.add_pr_curve(classes[class_index],\n",
    "                        tensorboard_preds,\n",
    "                        tensorboard_probs,\n",
    "                        global_step=global_step)\n",
    "    writer.close()\n",
    "\n",
    "# plot all the pr curves\n",
    "for i in range(len(classes)):\n",
    "    add_pr_curve_tensorboard(i, test_probs, test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cu110.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
