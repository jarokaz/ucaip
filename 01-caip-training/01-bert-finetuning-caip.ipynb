{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2020 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eHLV0D7Y5jtU"
   },
   "source": [
    "# BERT fine-tuning using AI Platform Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This notebook demonstrates how to use [AI Platform (Unified)](https://cloud.google.com/ai-platform-unified/docs/start/introduction-unified-platform) to run TensorFlow 2.x distributed training with GPUs. Both single node and multi-worker scenarios are covered.\n",
    "\n",
    "The ML scenario is BERT fine-tuning. You will use the text IMDB movie reviews database and the pre-trained BERT model from the [TensorFlow Hub](https://www.tensorflow.org/hub) to develop a text classification model for sentiment analysis.\n",
    "\n",
    "\n",
    "There are three types of AI Platform resources you can use to train custom models on AI Platform:\n",
    "\n",
    "- [Custom jobs](https://cloud.google.com/ai-platform-unified/docs/training/create-custom-job)\n",
    "- [Hyperparameter tuning jobs](https://cloud.google.com/ai-platform-unified/docs/training/using-hyperparameter-tuning)\n",
    "- [Training pipelines](https://cloud.google.com/ai-platform-unified/docs/training/create-training-pipeline)\n",
    "\n",
    "This sample focuses on [Custom jobs](https://cloud.google.com/ai-platform-unified/docs/training/create-custom-job) with [custom training containers](https://cloud.google.com/ai-platform-unified/docs/training/containers-overview).\n",
    "\n",
    "In the notebook, you will go through the following steps:\n",
    "\n",
    "- Converting the text IMDB database to the TFRecords format\n",
    "- Developing a custom training container\n",
    "- Configuring, submitting, and monitoring single node and multi-worker Custom training jobs\n",
    "\n",
    "\n",
    "### About BERT\n",
    "\n",
    "\n",
    "[BERT](https://arxiv.org/abs/1810.04805) and other Transformer encoder architectures have been wildly successful on a variety of tasks in NLP (natural language processing). They compute vector-space representations of natural language that are suitable for use in deep learning models. The BERT family of models uses the Transformer encoder architecture to process each token of input text in the full context of all tokens before and after, hence the name: Bidirectional Encoder Representations from Transformers. \n",
    "\n",
    "BERT models are usually pre-trained on a large corpus of text, then fine-tuned for specific tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7EUnXsZhAGF"
   },
   "source": [
    "## Setting up the environment\n",
    "\n",
    "### Setting up your notebook environment\n",
    "\n",
    "This notebook has been tested with [AI Platform Notebooks](https://cloud.google.com/ai-platform/notebooks/docs) configured with the standard TensorFlow 2.4 image.\n",
    "\n",
    "It may work on other environments as long as the similar hardware and software configuration is used.\n",
    "\n",
    "#### Provisioning an instance of AI Platform Notebooks\n",
    "\n",
    "To provision an instance of AI Platform Notebooks, follow the instructions in the [AI Platform Notebooks documentation](https://cloud.google.com/ai-platform/notebooks/docs/create-new). Configure  your instance with multiple GPUs and use the TensorFlow 2.4 image. \n",
    "\n",
    "#### Installing software pre-requisities\n",
    "\n",
    "In addition to standard packages pre-installed in the TensorFlow 2.4 image you need the following additional packages:\n",
    "- [AI Platform Python client library](https://cloud.google.com/ai-platform-unified/docs/start/client-libraries), and\n",
    "- [TensorFlow Model Garden library](https://github.com/tensorflow/models/tree/master/official)\n",
    "- [TF.Text](https://www.tensorflow.org/tutorials/tensorflow_text/intro)\n",
    "\n",
    "Use `pip` to install the libraries. You can run `pip` from a terminal window of your AI Platform Notebooks instance or execute the following cells. Make sure to restart the notebook after installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-cloud-aiplatform\n",
      "  Downloading google_cloud_aiplatform-0.5.1-py2.py3-none-any.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 8.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: google-cloud-storage<2.0.0dev,>=1.26.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (1.30.0)\n",
      "Requirement already satisfied: proto-plus>=1.10.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (1.13.0)\n",
      "Requirement already satisfied: google-api-core[grpc]<2.0.0dev,>=1.22.2 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (1.22.4)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (3.14.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (2.25.1)\n",
      "Requirement already satisfied: six>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (1.15.0)\n",
      "Requirement already satisfied: google-auth<2.0dev,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (1.26.1)\n",
      "Requirement already satisfied: setuptools>=34.0.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (53.0.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (1.52.0)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (2021.1)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.29.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (1.35.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (4.7)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (4.2.1)\n",
      "Requirement already satisfied: google-resumable-media<2.0dev,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage<2.0.0dev,>=1.26.0->google-cloud-aiplatform) (1.2.0)\n",
      "Requirement already satisfied: google-cloud-core<2.0dev,>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage<2.0.0dev,>=1.26.0->google-cloud-aiplatform) (1.3.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<2.0dev,>=0.6.0->google-cloud-storage<2.0.0dev,>=1.26.0->google-cloud-aiplatform) (1.1.2)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<2.0dev,>=0.6.0->google-cloud-storage<2.0.0dev,>=1.26.0->google-cloud-aiplatform) (1.14.4)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<2.0dev,>=0.6.0->google-cloud-storage<2.0.0dev,>=1.26.0->google-cloud-aiplatform) (2.20)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (0.4.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (1.26.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (4.0.0)\n",
      "Installing collected packages: google-cloud-aiplatform\n",
      "Successfully installed google-cloud-aiplatform-0.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U google-cloud-aiplatform --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf-models-official\n",
      "  Downloading tf_models_official-2.4.0-py2.py3-none-any.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 7.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-text\n",
      "  Downloading tensorflow_text-2.4.3-cp37-cp37m-manylinux1_x86_64.whl (3.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.4 MB 11.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tensorflow<2.5,>=2.4.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-text) (2.4.1)\n",
      "Requirement already satisfied: tensorflow-hub>=0.8.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-text) (0.9.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text) (2.4.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text) (1.1.0)\n",
      "Requirement already satisfied: numpy~=1.19.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text) (1.19.5)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text) (1.1.2)\n",
      "Requirement already satisfied: gast==0.3.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text) (0.3.3)\n",
      "Requirement already satisfied: tensorboard~=2.4 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text) (2.4.1)\n",
      "Requirement already satisfied: h5py~=2.10.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text) (2.10.0)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text) (0.2.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text) (3.14.0)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text) (3.3.0)\n",
      "Requirement already satisfied: six~=1.15.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text) (1.15.0)\n",
      "Requirement already satisfied: absl-py~=0.10 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text) (0.10.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text) (3.7.4.3)\n",
      "Requirement already satisfied: wheel~=0.35 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text) (0.36.2)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text) (1.12)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text) (1.12.1)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text) (1.6.3)\n",
      "Collecting grpcio~=1.32.0\n",
      "  Downloading grpcio-1.32.0-cp37-cp37m-manylinux2014_x86_64.whl (3.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8 MB 74.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text) (1.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text) (3.3.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text) (2.25.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text) (53.0.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text) (0.4.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text) (1.8.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text) (1.26.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text) (4.7)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text) (4.2.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text) (3.4.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text) (0.4.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text) (1.26.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text) (4.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text) (3.1.0)\n",
      "Requirement already satisfied: google-cloud-bigquery>=0.31.0 in /opt/conda/lib/python3.7/site-packages (from tf-models-official) (1.26.1)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.7/site-packages (from tf-models-official) (8.1.0)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 112.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting Cython\n",
      "  Downloading Cython-0.29.22-cp37-cp37m-manylinux1_x86_64.whl (2.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.0 MB 115.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: google-api-python-client>=1.6.7 in /opt/conda/lib/python3.7/site-packages (from tf-models-official) (1.12.8)\n",
      "Collecting dataclasses\n",
      "  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
      "Collecting gin-config\n",
      "  Downloading gin_config-0.4.0-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[K     |████████████████████████████████| 46 kB 7.5 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting py-cpuinfo>=3.3.0\n",
      "  Downloading py-cpuinfo-7.0.0.tar.gz (95 kB)\n",
      "\u001b[K     |████████████████████████████████| 95 kB 9.1 MB/s s eta 0:00:01\n",
      "\u001b[?25hCollecting seqeval\n",
      "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
      "\u001b[K     |████████████████████████████████| 43 kB 4.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: psutil>=5.4.3 in /opt/conda/lib/python3.7/site-packages (from tf-models-official) (5.8.0)\n",
      "Collecting opencv-python-headless\n",
      "  Downloading opencv_python_headless-4.5.1.48-cp37-cp37m-manylinux2014_x86_64.whl (37.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 37.6 MB 80.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tensorflow-datasets in /opt/conda/lib/python3.7/site-packages (from tf-models-official) (3.0.0)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from tf-models-official) (3.3.4)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from tf-models-official) (1.6.0)\n",
      "Collecting tensorflow-model-optimization>=0.4.1\n",
      "  Downloading tensorflow_model_optimization-0.5.0-py2.py3-none-any.whl (172 kB)\n",
      "\u001b[K     |████████████████████████████████| 172 kB 98.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-addons\n",
      "  Downloading tensorflow_addons-0.12.1-cp37-cp37m-manylinux2010_x86_64.whl (703 kB)\n",
      "\u001b[K     |████████████████████████████████| 703 kB 81.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pandas>=0.22.0 in /opt/conda/lib/python3.7/site-packages (from tf-models-official) (1.2.2)\n",
      "Collecting kaggle>=1.3.9\n",
      "  Downloading kaggle-1.5.10.tar.gz (59 kB)\n",
      "\u001b[K     |████████████████████████████████| 59 kB 14.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tf-slim>=1.1.0\n",
      "  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n",
      "\u001b[K     |████████████████████████████████| 352 kB 79.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: oauth2client in /opt/conda/lib/python3.7/site-packages (from tf-models-official) (4.1.3)\n",
      "Collecting pycocotools\n",
      "  Downloading pycocotools-2.0.2.tar.gz (23 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from tf-models-official) (5.4.1)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.6.7->tf-models-official) (3.0.1)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.6.7->tf-models-official) (0.0.4)\n",
      "Requirement already satisfied: google-api-core<2dev,>=1.21.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.6.7->tf-models-official) (1.22.4)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.6.7->tf-models-official) (0.19.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official) (1.52.0)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official) (2021.1)\n",
      "Requirement already satisfied: google-resumable-media<2.0dev,>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery>=0.31.0->tf-models-official) (1.2.0)\n",
      "Requirement already satisfied: google-cloud-core<2.0dev,>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery>=0.31.0->tf-models-official) (1.3.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<2.0dev,>=0.5.0->google-cloud-bigquery>=0.31.0->tf-models-official) (1.1.2)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<2.0dev,>=0.5.0->google-cloud-bigquery>=0.31.0->tf-models-official) (1.14.4)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<2.0dev,>=0.5.0->google-cloud-bigquery>=0.31.0->tf-models-official) (2.20)\n",
      "Requirement already satisfied: pyparsing<3,>=2.4.2 in /opt/conda/lib/python3.7/site-packages (from httplib2<1dev,>=0.15.0->google-api-python-client>=1.6.7->tf-models-official) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.7/site-packages (from kaggle>=1.3.9->tf-models-official) (2.8.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from kaggle>=1.3.9->tf-models-official) (4.56.2)\n",
      "Requirement already satisfied: python-slugify in /opt/conda/lib/python3.7/site-packages (from kaggle>=1.3.9->tf-models-official) (4.0.1)\n",
      "Requirement already satisfied: dm-tree~=0.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow-model-optimization>=0.4.1->tf-models-official) (0.1.5)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text) (3.4.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->tf-models-official) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->tf-models-official) (1.3.1)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /opt/conda/lib/python3.7/site-packages (from python-slugify->kaggle>=1.3.9->tf-models-official) (1.3)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.7/site-packages (from seqeval->tf-models-official) (0.24.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official) (1.0.1)\n",
      "Collecting typeguard>=2.7\n",
      "  Downloading typeguard-2.11.1-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: attrs>=18.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets->tf-models-official) (20.3.0)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets->tf-models-official) (0.3.1.1)\n",
      "Requirement already satisfied: tensorflow-metadata in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets->tf-models-official) (0.27.0)\n",
      "Requirement already satisfied: promise in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets->tf-models-official) (2.3)\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets->tf-models-official) (0.18.2)\n",
      "Building wheels for collected packages: kaggle, py-cpuinfo, pycocotools, seqeval\n",
      "  Building wheel for kaggle (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kaggle: filename=kaggle-1.5.10-py3-none-any.whl size=73268 sha256=aaa17af979e7c9dde11c9f01d933b949d4554c0c1039f6ab47ac6bd7a0cc8e37\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/ea/c5/fe/7e7fb5b3d1f150fac96188949b3d83d375a4c9df16ba557e52\n",
      "  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for py-cpuinfo: filename=py_cpuinfo-7.0.0-py3-none-any.whl size=20070 sha256=b094abdee4db3f1de568053365b8f384f8b35c69a8c9c67c4eed56c9cdae34b2\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/d7/59/0d/58c5e576d9192261fa3da00466eebe6f7a1ac1873a7ab1f2ce\n",
      "  Building wheel for pycocotools (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pycocotools: filename=pycocotools-2.0.2-cp37-cp37m-linux_x86_64.whl size=325542 sha256=3116ad9c6d6913b66c4dfb176586d2a1e222eda4ed381c8a78052ba8f7e6caf2\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/bc/cf/1b/e95c99c5f9d1648be3f500ca55e7ce55f24818b0f48336adaf\n",
      "  Building wheel for seqeval (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16170 sha256=a982bdeb6f93324953aec8f8e6162bb216ab110ffc451128a34697c6083f7378\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n",
      "Successfully built kaggle py-cpuinfo pycocotools seqeval\n",
      "Installing collected packages: grpcio, typeguard, Cython, tf-slim, tensorflow-model-optimization, tensorflow-addons, seqeval, sentencepiece, pycocotools, py-cpuinfo, opencv-python-headless, kaggle, gin-config, dataclasses, tf-models-official, tensorflow-text\n",
      "\u001b[33m  WARNING: The scripts cygdb, cython and cythonize are installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script cpuinfo is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script kaggle is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tfx 0.27.0 requires kubernetes<12,>=10.0.1, but you have kubernetes 12.0.1 which is incompatible.\n",
      "tfx 0.27.0 requires pyarrow<3,>=1, but you have pyarrow 3.0.0 which is incompatible.\n",
      "tensorflow-transform 0.27.0 requires pyarrow<3,>=1, but you have pyarrow 3.0.0 which is incompatible.\n",
      "tensorflow-model-analysis 0.27.0 requires pyarrow<3,>=1, but you have pyarrow 3.0.0 which is incompatible.\n",
      "tensorflow-data-validation 0.27.0 requires joblib<0.15,>=0.12, but you have joblib 1.0.1 which is incompatible.\n",
      "tensorflow-data-validation 0.27.0 requires pyarrow<3,>=1, but you have pyarrow 3.0.0 which is incompatible.\n",
      "explainable-ai-sdk 1.1.0 requires numpy<1.19.0, but you have numpy 1.19.5 which is incompatible.\n",
      "apache-beam 2.27.0 requires httplib2<0.18.0,>=0.8, but you have httplib2 0.19.0 which is incompatible.\n",
      "apache-beam 2.27.0 requires pyarrow<3.0.0,>=0.15.1, but you have pyarrow 3.0.0 which is incompatible.\u001b[0m\n",
      "Successfully installed Cython-0.29.22 dataclasses-0.6 gin-config-0.4.0 grpcio-1.32.0 kaggle-1.5.10 opencv-python-headless-4.5.1.48 py-cpuinfo-7.0.0 pycocotools-2.0.2 sentencepiece-0.1.95 seqeval-1.2.2 tensorflow-addons-0.12.1 tensorflow-model-optimization-0.5.0 tensorflow-text-2.4.3 tf-models-official-2.4.0 tf-slim-1.1.0 typeguard-2.11.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tf-models-official tensorflow-text --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you installed the packages from within the notebookd make sure to restart the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BF1j6f9HApxa"
   },
   "source": [
    "### Setting up your GCP project\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "1. [Select or create a GCP project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
    "\n",
    "2. [Make sure that billing is enabled for your project.](https://cloud.google.com/billing/docs/how-to/modify-project)\n",
    "\n",
    "3. [Enable the AI Platform APIs, Compute Engine APIs and Container Registry API.](https://console.cloud.google.com/flows/enableapi?apiid=ml.googleapis.com,compute_component,containerregistry.googleapis.com)\n",
    "\n",
    "4. [Google Cloud SDK](https://cloud.google.com/sdk) is already installed in AI Platform Notebooks.\n",
    "\n",
    "5. Enter your project ID in the cell below. Then run the  cell to make sure the\n",
    "Cloud SDK uses the right project for all the commands in this notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set your project ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "Oe_QtcD8xa3y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "\u001b[1;33mWARNING:\u001b[0m You do not appear to have access to project [jk-demos] or it does not exist.\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ID = 'jk-demos'\n",
    "\n",
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F4t4fNIrxa3z"
   },
   "source": [
    "#### Set the default region\n",
    "\n",
    "You can also change the `REGION` variable, which is used for operations\n",
    "throughout the rest of this notebook.  Below are regions supported for AI Platform (Unified). We recommend when possible, to choose the region closest to you. \n",
    "\n",
    "- Americas: `us-central1`\n",
    "- Europe: `europe-west4`\n",
    "- Asia Pacific: `asia-east1`\n",
    "\n",
    "You can not use a Multi-Regional Storage bucket for training with AI Platform. Not all regions provide support for all AI Platform services. For the lastest support per region, see [Region support for AI Platform (Unified) services](https://cloud.google.com/ai-platform-unified/docs/general/locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "YMA5Tb-8xa30"
   },
   "outputs": [],
   "source": [
    "REGION = 'us-central1' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgPO1eR3CYjk"
   },
   "source": [
    "#### Create a Cloud Storage bucket\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "In this tutorial, your training job retrieves data and  saves the artifacts created during the job, including\n",
    "a trained model, checkpoints, and the TensorBoard logs, into a Google Cloud storage bucket. \n",
    "\n",
    "Set the name of your Cloud Storage bucket below. It must be unique across all\n",
    "Cloud Storage buckets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "MzGDU7TWdts_"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"jk-demos-bucket\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EcIXiGsCePi"
   },
   "source": [
    "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "NIq7R4HZCfIc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://jk-demos-bucket/...\n",
      "ServiceException: 409 A Cloud Storage bucket named 'jk-demos-bucket' already exists. Try another name. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization.\n"
     ]
    }
   ],
   "source": [
    "! gsutil mb -l $REGION gs://$BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucvCsknMCims"
   },
   "source": [
    "Finally, validate access to your Cloud Storage bucket by examining its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "vhOb7YnwClBb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 gs://jk-demos-bucket/data/\n",
      "                                 gs://jk-demos-bucket/test_run/\n",
      "                                 gs://jk-demos-bucket/tfrecords/\n"
     ]
    }
   ],
   "source": [
    "! gsutil ls -al gs://$BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OxBoSThVxa34"
   },
   "source": [
    "#### Set AI Platform (Unified) constants\n",
    "\n",
    "Let's now setup some constants for AI Platform (Unified):\n",
    "\n",
    "- `API_ENDPOINT`: The AI Platform (Unified) API service endpoint for dataset, model, job, pipeline and endpoint services.\n",
    "- `API_PREDICT_ENDPOINT`: The AI Platform (Unified) API service endpoint for prediction.\n",
    "- `PARENT`: The AI Platform (Unified) location root path for dataset, model and endpoint resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "dZtcw173xa34"
   },
   "outputs": [],
   "source": [
    "# API Endpoint\n",
    "API_ENDPOINT = \"{}-aiplatform.googleapis.com\".format(REGION)\n",
    "API_PREDICT_ENDPOINT = \"{}-prediction-aiplatform.googleapis.com\".format(REGION)\n",
    "\n",
    "# AI Platform (Unified) location root path for your dataset, model and endpoint resources\n",
    "PARENT = \"projects/\" + PROJECT_ID + \"/locations/\" + REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "#### Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "pRUOFELefqf1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from datetime import datetime\n",
    "from google.protobuf import json_format\n",
    "from google.protobuf.struct_pb2 import Value\n",
    "\n",
    "from google.cloud.aiplatform import gapic as aip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E6ppE7imft-y"
   },
   "source": [
    "### Preparing data\n",
    "\n",
    "In this section, you will convert the original IMDB dataset that is in plain text into the [TFRecord](https://www.tensorflow.org/tutorials/load_data/tfrecord) format. The TFRecord format is recommended for high performance input pipelines that are critical in large scale training scenarios like BERT pre-training and fine-tuning. The TFRecord format works well with the [tf.data API](https://www.tensorflow.org/guide/data) used to implement input pipelines in this sample.\n",
    "\n",
    "After the TFRecord files are created, you will copy them to a GCS storage bucket. In most distributed training scenarios, training data needs to be located in a shared storage location.\n",
    "\n",
    "#### Download the IMDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "84131840/84125825 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "local_dir = os.path.expanduser('~')\n",
    "local_dir = f'{local_dir}/datasets'\n",
    "\n",
    "if tf.io.gfile.exists(local_dir):\n",
    "    tf.io.gfile.rmtree(local_dir)\n",
    "tf.io.gfile.makedirs(local_dir)\n",
    "\n",
    "url = 'https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "local_path = f'{local_dir}/aclImdb_v1.tar.gz'\n",
    "\n",
    "dataset = tf.keras.utils.get_file(local_path, url,\n",
    "                                  untar=True, \n",
    "                                  cache_dir=local_dir,\n",
    "                                  cache_subdir='.'\n",
    "                                  )\n",
    "dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\n",
    "\n",
    "train_dir = os.path.join(dataset_dir, 'train')\n",
    "\n",
    "# remove unused folders to make it easier to load the data\n",
    "remove_dir = os.path.join(train_dir, 'unsup')\n",
    "shutil.rmtree(remove_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert the IMDB dataset to TFRecords files\n",
    "\n",
    "##### Create training, validation and testing splits from IMDB text files\n",
    "\n",
    "The IMDB dataset has already been divided into train and test, but it lacks a validation set. We will create a validation set using an 80:20 split of the training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_splits(train_dir, test_dir, val_split, seed):\n",
    "    \n",
    "    train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "        train_dir,\n",
    "        validation_split=val_split,\n",
    "        subset='training',\n",
    "        seed=seed)\n",
    "\n",
    "    class_names = train_ds.class_names\n",
    "    \n",
    "    train_ds = train_ds.unbatch()\n",
    "\n",
    "    val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "        train_dir,\n",
    "        validation_split=val_split,\n",
    "        subset='validation',\n",
    "        seed=seed).unbatch()\n",
    "\n",
    "    test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "        test_dir).unbatch()\n",
    "\n",
    "    return train_ds, val_ds, test_ds, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 20000 files for training.\n",
      "Found 25000 files belonging to 2 classes.\n",
      "Using 5000 files for validation.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "val_split = 0.2\n",
    "test_dir = f'{dataset_dir}/test'\n",
    "\n",
    "train_ds, val_ds, test_ds, class_names = (\n",
    "    create_splits(train_dir, test_dir, val_split, seed)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inspect a couple of examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: b'\"Pandemonium\" is a horror movie spoof that comes off more stupid than funny. Believe me when I tell you, I love comedies. Especially comedy spoofs. \"Airplane\", \"The Naked Gun\" trilogy, \"Blazing Saddles\", \"High Anxiety\", and \"Spaceballs\" are some of my favorite comedies that spoof a particular genre. \"Pandemonium\" is not up there with those films. Most of the scenes in this movie had me sitting there in stunned silence because the movie wasn\\'t all that funny. There are a few laughs in the film, but when you watch a comedy, you expect to laugh a lot more than a few times and that\\'s all this film has going for it. Geez, \"Scream\" had more laughs than this film and that was more of a horror film. How bizarre is that?<br /><br />*1/2 (out of four)'\n",
      "Label : 0 (neg)\n",
      "Review: b\"David Mamet is a very interesting and a very un-equal director. His first movie 'House of Games' was the one I liked best, and it set a series of films with characters whose perspective of life changes as they get into complicated situations, and so does the perspective of the viewer.<br /><br />So is 'Homicide' which from the title tries to set the mind of the viewer to the usual crime drama. The principal characters are two cops, one Jewish and one Irish who deal with a racially charged area. The murder of an old Jewish shop owner who proves to be an ancient veteran of the Israeli Independence war triggers the Jewish identity in the mind and heart of the Jewish detective.<br /><br />This is were the flaws of the film are the more obvious. The process of awakening is theatrical and hard to believe, the group of Jewish militants is operatic, and the way the detective eventually walks to the final violent confrontation is pathetic. The end of the film itself is Mamet-like smart, but disappoints from a human emotional perspective.<br /><br />Joe Mantegna and William Macy give strong performances, but the flaws of the story are too evident to be easily compensated.\"\n",
      "Label : 0 (neg)\n"
     ]
    }
   ],
   "source": [
    "for text, label in train_ds.take(2):\n",
    "    print(f'Review: {text.numpy()}')\n",
    "    label = label.numpy()\n",
    "    print(f'Label : {label} ({class_names[label]})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare tf.Example serialization routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_example(text_fragment, label):\n",
    "    \"\"\"Serializes text fragment and label in tf.Example.\"\"\"\n",
    "    \n",
    "    def _bytes_feature(value):\n",
    "        \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "        if isinstance(value, type(tf.constant(0))):\n",
    "            value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
    "        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "    def _int64_feature(value):\n",
    "        \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "        return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "    \n",
    "    feature = {\n",
    "        'text_fragment': _bytes_feature(text_fragment),\n",
    "        'label': _int64_feature(label)\n",
    "    }\n",
    "    \n",
    "    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    return example_proto.SerializeToString()\n",
    "    \n",
    "def tf_serialize_example(text_fragment, label):\n",
    "  tf_string = tf.py_function(\n",
    "    serialize_example,\n",
    "    (text_fragment, label), \n",
    "    tf.string)      \n",
    "  return tf.reshape(tf_string, ()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Write TFRecords files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfrecords_folder = '{}/tfrecords'.format(os.path.expanduser('~'))\n",
    "if tf.io.gfile.exists(tfrecords_folder):\n",
    "    tf.io.gfile.rmtree(tfrecords_folder)\n",
    "tf.io.gfile.makedirs(tfrecords_folder)\n",
    "\n",
    "filenames = ['train.tfrecords', 'valid.tfrecords', 'test.tfrecords']\n",
    "for file_name, dataset in zip(filenames, [train_ds, val_ds, test_ds]):\n",
    "    writer = tf.data.experimental.TFRecordWriter(os.path.join(tfrecords_folder, file_name))\n",
    "    writer.write(dataset.map(tf_serialize_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Double check that you can read the created TFRecord files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'\\n\\xc4\\x0f\\n\\x0e\\n\\x05label\\x12\\x05\\x1a\\x03\\n\\x01\\x00\\n\\xb1\\x0f\\n\\rtext_fragment\\x12\\x9f\\x0f\\n\\x9c\\x0f\\n\\x99\\x0fI once had a conversation with my parents who told me British cinema goers in the 1940s and 50s would check to see a film\\'s country of origin before going to see it . It didn\\'t matter what the plot was or who was in it , if it was an American movie people would want to see it and if it was British people wouldn\\'t want to see it . This might sound like a ridiculous generalisation but after seeing THE ASTONISHED HEART I can understand why people in those days preferred American cinema to the home grown variety Back in the 1940s <br /><br />British equity was devoid of working class members and it shows in this movie . Everyone speaks in an English lad dee daa upper class accent that makes the British Royal Family sound like working class scum and what this does is alienate a large amount of a potential British audience who would no doubt prefer to be watching Jimmy Cagney in WHITE HEAT because people would have , If not related to then certainly empathised with a violent gangster in cinematic terms more than some high class English shrink in 1949 . That\\'s entertainment , the reason people go to cinemas . Even the characters names seem bizarre - Leonora ! How many British people were named Leonora in 1949 ? And the protagonists drink cocktails . And they use words like \" Austere \" . You do get the feeling that this wasn\\'t marketed for a 1949 mainstream British audience . But why should it if the majority of British cinema goers were queuing up at cinemas to watch far more entertaining American imports ? <br /><br />Watching THE ATSONISHED HEART in 2005 I was astonished how dated everything was , in fact it\\'s so dated I thought maybe it might be a spoof from THE HARRY ENDFIELD SHOW . What didn\\'t astonish me was the fact that these types of movie came close to sinking the British film industry , an industry that didn\\'t pick up until American money invested in crowd pleasers like ZULU , ALFIE and the James Bond movies', shape=(), dtype=string)\n",
      "tf.Tensor(b\"\\n\\xb2\\x05\\n\\x9f\\x05\\n\\rtext_fragment\\x12\\x8d\\x05\\n\\x8a\\x05\\n\\x87\\x05It seems a lot of Europeans and Americans see Indian movies for the wrong reason; I see some people are complaining that this movie did not have any dance sequence! A class apart from their Hindi counterparts, Bengali movies tend to be more realistic. Rituparno Ghosh is one of the best young directors in India, being widely known for his choice of subjects for the movies and the strength of his scripts. 'Chokher bali' is a perfect example. A faithful adaptation of the Nobel laureate Tagore's novel dealing with the pursuit of sexual pleasure of a Bengali widow, the director gives a new dimension to the much acclaimed and controversial work.\\n\\x0e\\n\\x05label\\x12\\x05\\x1a\\x03\\n\\x01\\x01\", shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for record in tf.data.TFRecordDataset([os.path.join(tfrecords_folder, file_name)]).take(2):\n",
    "    print(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Copy the created TFRecord files to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file:///home/jupyter/tfrecords/train.tfrecords [Content-Type=application/octet-stream]...\n",
      "- [1 files][ 26.5 MiB/ 26.5 MiB]                                                \n",
      "Operation completed over 1 objects/26.5 MiB.                                     \n",
      "Copying file:///home/jupyter/tfrecords/valid.tfrecords [Content-Type=application/octet-stream]...\n",
      "/ [1 files][  6.6 MiB/  6.6 MiB]                                                \n",
      "Operation completed over 1 objects/6.6 MiB.                                      \n",
      "Copying file:///home/jupyter/tfrecords/test.tfrecords [Content-Type=application/octet-stream]...\n",
      "- [1 files][ 32.3 MiB/ 32.3 MiB]                                                \n",
      "Operation completed over 1 objects/32.3 MiB.                                     \n"
     ]
    }
   ],
   "source": [
    "gcs_paths = [f'gs://{BUCKET_NAME}/tfrecords/train',\n",
    "             f'gs://{BUCKET_NAME}/tfrecords/valid',\n",
    "             f'gs://{BUCKET_NAME}/tfrecords/test']\n",
    "\n",
    "for filename, gcs_path in zip(filenames, gcs_paths):\n",
    "    local_file_path = os.path.join(tfrecords_folder, filename)\n",
    "    gcs_file_path = f'{gcs_path}/{filename}'\n",
    "    !gsutil cp {local_file_path} {gcs_file_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the training container image\n",
    "\n",
    "\n",
    "There are two ways of packaging your training code for AI Platform Custom jobs. \n",
    "\n",
    "- **Use a Google Cloud prebuilt container**. If you use a prebuilt container, you will additionally specify a Python package to install into the container image. This Python package contains your code for training a custom model.\n",
    "\n",
    "- **Use your own custom container image**. If you use your own container, the container needs to contain your code plus all the dependencies..\n",
    "\n",
    "In this sample, we are using a custom container.\n",
    "\n",
    "To create a custom training container you need to define a Python training module and package it in a container image together with all the required dependencies.\n",
    "\n",
    "We will use the standard [Deep Learning Containers](https://cloud.google.com/ai-platform/deep-learning-containers/docs) image as a base image for the custom traininer container image. Specifically we are going to use the `gcr.io/deeplearning-platform-release/tf2-gpu.2-4` image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BASE_IMAGE = 'gcr.io/deeplearning-platform-release/tf2-gpu.2-4'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fn-LlmWGxa35"
   },
   "source": [
    "\n",
    "### Create the training module\n",
    "\n",
    "A custom training image encapsulates you training code. You can structure your code in anyway you want as long as you can invoke it through a standard docker container interface. \n",
    "\n",
    "In this sample, the training code is encapsulated in a single Python module - `task.py`. The runtime parameters can be passed as command line arguments.  The below section summarizes key design decisions taken when designing the training regime.\n",
    "\n",
    "#### Model design\n",
    "\n",
    "This sample implements a simple classification model using pre-trained BERT components from TensorFlow Hub. Specifically a classic BERT architecture with L=12 hidden layers, a hidden size of H=768, and A=12 attention heads is used. This [TF Hub model](https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3) uses the implementatio of BERT from the [TensorFlow Model Garden repository](https://github.com/tensorflow/models/tree/master/official/nlp/bert). \n",
    "\n",
    "Text inputs need to be transformed to numeric token ids and arranged in several Tensors before being input to BERT. TensorFlow Hub provides a matching preprocessing model for each of the BERT models, which implements this transformation using TF ops from the TF.text library. It is not necessary to run pure Python code outside your TensorFlow model to preprocess text.\n",
    "\n",
    "The model implemented in the script embedds the preprocessing model from TF Hub as a Keras layer.\n",
    "\n",
    "Since this is a binary classification problem and the model outputs a probability (a single-unit layer), the model uses `tf.keras.losses.BinaryCrossentropy` loss function and `tf.metrics.BinaryAccuracy` metric.\n",
    "\n",
    "The sample uses the same optimizer that BERT was originally trained with: the \"Adaptive Moments\" (Adam). This optimizer minimizes the prediction loss and does regularization by weight decay (not using moments), which is also known as [AdamW](https://arxiv.org/abs/1711.05101).\n",
    "\n",
    "For the learning rate (`init_lr`), the same schedule as BERT pre-training is used: linear decay of a notional initial learning rate, prefixed with a linear warm-up phase over the first 10% of training steps (`num_warmup_steps`). In line with the BERT paper, the initial learning rate is smaller for fine-tuning (best of 5e-5, 3e-5, 2e-5).\n",
    "\n",
    "#### Input pipelines\n",
    "\n",
    "The training code utilizes `tf.data` to implement input pipelines. The common techniques for optimizing performance - caching, prefetching - are applied. To better support distributed training the script allows for explicit configuration of [Auto Sharding Policy](https://www.tensorflow.org/tutorials/distribute/input).\n",
    "\n",
    "#### Fault tolerance\n",
    "\n",
    "The script utilizes the [`tf.keras.callbacks.experimental.BackupAndRestore`](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras#backupandrestore_callback) callback for resilience from failures during training. The callback provides fault tolerance, by backing up the model and current epoch number in a temporary checkpoint file. This is done at the endo of each epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "R4yEjEt8xa36"
   },
   "outputs": [],
   "source": [
    "! rm -rf trainer\n",
    "! mkdir trainer\n",
    "! touch trainer/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile trainer/task.py\n",
    "\n",
    "\n",
    "# Copyright 2021 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#            http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from absl import logging\n",
    "from official.nlp import optimization \n",
    "\n",
    "\n",
    "TFHUB_HANDLE_ENCODER = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3'\n",
    "TFHUB_HANDLE_PREPROCESS = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\n",
    "LOCAL_TB_FOLDER = '/tmp/logs'\n",
    "LOCAL_SAVED_MODEL_DIR = '/tmp/saved_model'\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_integer('steps_per_epoch', 625, 'Steps per training epoch')\n",
    "flags.DEFINE_integer('eval_steps', 150, 'Evaluation steps')\n",
    "flags.DEFINE_integer('epochs', 2, 'Nubmer of epochs')\n",
    "flags.DEFINE_integer('per_replica_batch_size', 32, 'Per replica batch size')\n",
    "flags.DEFINE_string('training_data_path', 'gs://jk-demos-bucket/tfrecords/train', 'Training data GCS path')\n",
    "flags.DEFINE_string('validation_data_path', 'gs://jk-demos-bucket/tfrecords/valid', 'Validation data GCS path')\n",
    "flags.DEFINE_string('testing_data_path', 'gs://jk-demos-bucket/data/imdb/test', 'Testing data GCS path')\n",
    "\n",
    "flags.DEFINE_string('job_dir', 'gs://jk-demos-bucket/jobs', 'A base GCS path for jobs')\n",
    "flags.DEFINE_enum('strategy', 'multiworker', ['mirrored', 'multiworker'], 'Distribution strategy')\n",
    "flags.DEFINE_enum('auto_shard_policy', 'auto', ['auto', 'data', 'file', 'off'], 'Dataset sharing strategy')\n",
    "\n",
    "\n",
    "\n",
    "auto_shard_policy = {\n",
    "    'auto': tf.data.experimental.AutoShardPolicy.AUTO,\n",
    "    'data': tf.data.experimental.AutoShardPolicy.DATA,\n",
    "    'file': tf.data.experimental.AutoShardPolicy.FILE,\n",
    "    'off': tf.data.experimental.AutoShardPolicy.OFF,\n",
    "}\n",
    "\n",
    "\n",
    "def create_unbatched_dataset(tfrecords_folder):\n",
    "    \"\"\"Creates an unbatched dataset in the format required by the \n",
    "       sentiment analysis model from the folder with TFrecords files.\"\"\"\n",
    "    \n",
    "    feature_description = {\n",
    "        'text_fragment': tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
    "        'label': tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
    "    }\n",
    "\n",
    "    def _parse_function(example_proto):\n",
    "        parsed_example = tf.io.parse_single_example(example_proto, feature_description)\n",
    "        return parsed_example['text_fragment'], parsed_example['label']\n",
    "  \n",
    "    file_paths = [f'{tfrecords_folder}/{file_path}' for file_path in tf.io.gfile.listdir(tfrecords_folder)]\n",
    "    dataset = tf.data.TFRecordDataset(file_paths)\n",
    "    dataset = dataset.map(_parse_function)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def configure_dataset(ds, auto_shard_policy):\n",
    "    \"\"\"\n",
    "    Optimizes the performance of a dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    options = tf.data.Options()\n",
    "    options.experimental_distribute.auto_shard_policy = (\n",
    "        auto_shard_policy\n",
    "    )\n",
    "    \n",
    "    ds = ds.repeat(-1).cache()\n",
    "    ds = ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    ds = ds.with_options(options)\n",
    "    return ds\n",
    "\n",
    "\n",
    "def create_input_pipelines(train_dir, valid_dir, test_dir, batch_size, auto_shard_policy):\n",
    "    \"\"\"Creates input pipelines from Imdb dataset.\"\"\"\n",
    "    \n",
    "    train_ds = create_unbatched_dataset(train_dir)\n",
    "    train_ds = train_ds.batch(batch_size)\n",
    "    train_ds = configure_dataset(train_ds, auto_shard_policy)\n",
    "    \n",
    "    valid_ds = create_unbatched_dataset(valid_dir)\n",
    "    valid_ds = valid_ds.batch(batch_size)\n",
    "    valid_ds = configure_dataset(valid_ds, auto_shard_policy)\n",
    "    \n",
    "    test_ds = create_unbatched_dataset(test_dir)\n",
    "    test_ds = test_ds.batch(batch_size)\n",
    "    test_ds = configure_dataset(test_ds, auto_shard_policy)\n",
    "\n",
    "    return train_ds, valid_ds, test_ds\n",
    "\n",
    "\n",
    "def build_classifier_model(tfhub_handle_preprocess, tfhub_handle_encoder):\n",
    "    \"\"\"Builds a simple binary classification model with BERT trunk.\"\"\"\n",
    "    \n",
    "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
    "    encoder_inputs = preprocessing_layer(text_input)\n",
    "    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
    "    outputs = encoder(encoder_inputs)\n",
    "    net = outputs['pooled_output']\n",
    "    net = tf.keras.layers.Dropout(0.1)(net)\n",
    "    net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n",
    "    \n",
    "    return tf.keras.Model(text_input, net)\n",
    "\n",
    "\n",
    "def copy_tensorboard_logs(local_path: str, gcs_path: str):\n",
    "    \"\"\"Copies Tensorboard logs from a local dir to a GCS location.\n",
    "    \n",
    "    After training, batch copy Tensorboard logs locally to a GCS location. This can result\n",
    "    in faster pipeline runtimes over streaming logs per batch to GCS that can get bottlenecked\n",
    "    when streaming large volumes.\n",
    "    \n",
    "    Args:\n",
    "      local_path: local filesystem directory uri.\n",
    "      gcs_path: cloud filesystem directory uri.\n",
    "    Returns:\n",
    "      None.\n",
    "    \"\"\"\n",
    "    pattern = '{}/*/events.out.tfevents.*'.format(local_path)\n",
    "    local_files = tf.io.gfile.glob(pattern)\n",
    "    gcs_log_files = [local_file.replace(local_path, gcs_path) for local_file in local_files]\n",
    "    for local_file, gcs_file in zip(local_files, gcs_log_files):\n",
    "        tf.io.gfile.copy(local_file, gcs_file)\n",
    "\n",
    "\n",
    "def main(argv):\n",
    "    del argv\n",
    "    \n",
    "    def _is_chief(task_type, task_id):\n",
    "        return ((task_type == 'chief' or task_type == 'worker') and task_id == 0) or task_type is None\n",
    "        \n",
    "    \n",
    "    logging.info('Setting up training.')\n",
    "    logging.info('   epochs: {}'.format(FLAGS.epochs))\n",
    "    logging.info('   steps_per_epoch: {}'.format(FLAGS.steps_per_epoch))\n",
    "    logging.info('   eval_steps: {}'.format(FLAGS.eval_steps))\n",
    "    logging.info('   strategy: {}'.format(FLAGS.strategy))\n",
    "    \n",
    "    if FLAGS.strategy == 'mirrored':\n",
    "        strategy = tf.distribute.MirroredStrategy()\n",
    "    else:\n",
    "        strategy = tf.distribute.MultiWorkerMirroredStrategy()\n",
    "        \n",
    "    if strategy.cluster_resolver:    \n",
    "        task_type, task_id = (strategy.cluster_resolver.task_type,\n",
    "                              strategy.cluster_resolver.task_id)\n",
    "    else:\n",
    "        task_type, task_id =(None, None)\n",
    "        \n",
    "    \n",
    "    global_batch_size = (strategy.num_replicas_in_sync *\n",
    "                         FLAGS.per_replica_batch_size)\n",
    "    \n",
    "    \n",
    "    train_ds, valid_ds, test_ds = create_input_pipelines(\n",
    "        FLAGS.training_data_path,\n",
    "        FLAGS.validation_data_path,\n",
    "        FLAGS.testing_data_path,\n",
    "        global_batch_size,\n",
    "        auto_shard_policy[FLAGS.auto_shard_policy])\n",
    "        \n",
    "    num_train_steps = FLAGS.steps_per_epoch * FLAGS.epochs\n",
    "    num_warmup_steps = int(0.1*num_train_steps)\n",
    "    init_lr = 3e-5\n",
    "    \n",
    "    with strategy.scope():\n",
    "        model = build_classifier_model(TFHUB_HANDLE_PREPROCESS, TFHUB_HANDLE_ENCODER)\n",
    "        loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "        metrics = tf.metrics.BinaryAccuracy()\n",
    "        optimizer = optimization.create_optimizer(\n",
    "            init_lr=init_lr,\n",
    "            num_train_steps=num_train_steps,\n",
    "            num_warmup_steps=num_warmup_steps,\n",
    "            optimizer_type='adamw')\n",
    "\n",
    "        model.compile(optimizer=optimizer,\n",
    "                      loss=loss,\n",
    "                      metrics=metrics)\n",
    "        \n",
    "    # Configure BackupAndRestore callback\n",
    "    backup_dir = '{}/backupandrestore'.format(FLAGS.job_dir)\n",
    "    callbacks = [tf.keras.callbacks.experimental.BackupAndRestore(backup_dir=backup_dir)]\n",
    "    \n",
    "    # Configure TensorBoard callback on Chief\n",
    "    if _is_chief(task_type, task_id):\n",
    "        callbacks.append(tf.keras.callbacks.TensorBoard(\n",
    "            log_dir=LOCAL_TB_FOLDER, update_freq='batch'))\n",
    "    \n",
    "    logging.info('Starting training ...')\n",
    "    \n",
    "    history = model.fit(x=train_ds,\n",
    "                        validation_data=valid_ds,\n",
    "                        steps_per_epoch=FLAGS.steps_per_epoch,\n",
    "                        validation_steps=FLAGS.eval_steps,\n",
    "                        epochs=FLAGS.epochs,\n",
    "                        callbacks=callbacks)\n",
    "\n",
    "    if _is_chief(task_type, task_id):\n",
    "        # Copy tensorboard logs to GCS\n",
    "        tb_logs = '{}/tb_logs'.format(FLAGS.job_dir)\n",
    "        logging.info('Copying TensorBoard logs to: {}'.format(tb_logs))\n",
    "        copy_tensorboard_logs(LOCAL_TB_FOLDER, tb_logs)\n",
    "        saved_model_dir = '{}/saved_model'.format(FLAGS.job_dir)\n",
    "    else:\n",
    "        saved_model_dir = LOCAL_SAVED_MODEL_DIR\n",
    "        \n",
    "    # Save trained model\n",
    "    saved_model_dir = '{}/saved_model'.format(FLAGS.job_dir)\n",
    "    logging.info('Training completed. Saving the trained model to: {}'.format(saved_model_dir))\n",
    "    model.save(saved_model_dir)\n",
    "    #tf.saved_model.save(model, saved_model_dir)\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    logging.set_verbosity(logging.INFO)\n",
    "    app.run(main)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a docker file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_IMAGE = f'gcr.io/{PROJECT_ID}/imdb_bert'\n",
    "\n",
    "dockerfile = f'''\n",
    "FROM {TRAIN_BASE_IMAGE}\n",
    "\n",
    "RUN pip install pip install tf-models-official tensorflow-text \n",
    "\n",
    "WORKDIR /\n",
    "\n",
    "# Copies the trainer code to the docker image.\n",
    "COPY trainer /trainer\n",
    "\n",
    "# Sets up the entry point to invoke the trainer.\n",
    "ENTRYPOINT [\"python\", \"-m\", \"trainer.task\"]\n",
    "'''\n",
    "\n",
    "with open('Dockerfile', 'w') as f:\n",
    "    f.write(dockerfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a container image and upload it to your Container Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  29.18MB\n",
      "Step 1/5 : FROM gcr.io/deeplearning-platform-release/tf2-gpu.2-4\n",
      "latest: Pulling from deeplearning-platform-release/tf2-gpu.2-4\n",
      "\n",
      "\u001b[1B57c49d0f: Pulling fs layer \n",
      "\u001b[1B40447d26: Pulling fs layer \n",
      "\u001b[1B2f862619: Pulling fs layer \n",
      "\u001b[1B278deddf: Pulling fs layer \n",
      "\u001b[1B80049843: Pulling fs layer \n",
      "\u001b[1B556b2329: Pulling fs layer \n",
      "\u001b[1Ba0c97a55: Pulling fs layer \n",
      "\u001b[1B78bd0b24: Pulling fs layer \n",
      "\u001b[1B6c31766d: Pulling fs layer \n",
      "\u001b[1Bba07e80d: Pulling fs layer \n",
      "\u001b[1Bfcc52fd9: Pulling fs layer \n",
      "\u001b[1Bfc936b5a: Pulling fs layer \n",
      "\u001b[1B883e2cff: Pulling fs layer \n",
      "\u001b[1B4b93e8e5: Pulling fs layer \n",
      "\u001b[1B4e709e13: Pulling fs layer \n",
      "\u001b[1Be868c172: Pulling fs layer \n",
      "\u001b[1B4ff2ee46: Pulling fs layer \n",
      "\u001b[1B576f4ffd: Pulling fs layer \n",
      "\u001b[1B1b43a473: Pulling fs layer \n",
      "\u001b[1B4258e91f: Pulling fs layer \n",
      "\u001b[1Bf1864c27: Pulling fs layer \n",
      "\u001b[1B6ff8756c: Pulling fs layer \n",
      "\u001b[1Ba82d4819: Pulling fs layer \n",
      "\u001b[1B4d9491c9: Pulling fs layer \n",
      "\u001b[1Bb66220c8: Pulling fs layer \n",
      "\u001b[1B31b11760: Pulling fs layer \n",
      "\u001b[1B96c7f038: Pull complete 0.4kB/670.4kBB27A\u001b[2K\u001b[24A\u001b[2K\u001b[24A\u001b[2K\u001b[27A\u001b[2K\u001b[27A\u001b[2K\u001b[23A\u001b[2K\u001b[27A\u001b[2K\u001b[21A\u001b[2K\u001b[27A\u001b[2K\u001b[19A\u001b[2K\u001b[27A\u001b[2K\u001b[19A\u001b[2K\u001b[27A\u001b[2K\u001b[19A\u001b[2K\u001b[20A\u001b[2K\u001b[19A\u001b[2K\u001b[20A\u001b[2K\u001b[19A\u001b[2K\u001b[20A\u001b[2K\u001b[19A\u001b[2K\u001b[20A\u001b[2K\u001b[19A\u001b[2K\u001b[20A\u001b[2K\u001b[19A\u001b[2K\u001b[20A\u001b[2K\u001b[19A\u001b[2K\u001b[20A\u001b[2K\u001b[19A\u001b[2K\u001b[20A\u001b[2K\u001b[19A\u001b[2K\u001b[20A\u001b[2K\u001b[21A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[19A\u001b[2K\u001b[21A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[23A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[20A\u001b[2K\u001b[19A\u001b[2K\u001b[20A\u001b[2K\u001b[21A\u001b[2K\u001b[20A\u001b[2K\u001b[19A\u001b[2K\u001b[20A\u001b[2K\u001b[21A\u001b[2K\u001b[20A\u001b[2K\u001b[19A\u001b[2K\u001b[21A\u001b[2K\u001b[19A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[19A\u001b[2K\u001b[21A\u001b[2K\u001b[19A\u001b[2K\u001b[21A\u001b[2K\u001b[19A\u001b[2K\u001b[20A\u001b[2K\u001b[19A\u001b[2K\u001b[21A\u001b[2K\u001b[19A\u001b[2K\u001b[21A\u001b[2K\u001b[20A\u001b[2K\u001b[21A\u001b[2K\u001b[20A\u001b[2K\u001b[19A\u001b[2K\u001b[21A\u001b[2K\u001b[19A\u001b[2K\u001b[21A\u001b[2K\u001b[19A\u001b[2K\u001b[21A\u001b[2K\u001b[19A\u001b[2K\u001b[21A\u001b[2K\u001b[19A\u001b[2K\u001b[21A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[20A\u001b[2K\u001b[19A\u001b[2K\u001b[20A\u001b[2K\u001b[21A\u001b[2K\u001b[20A\u001b[2K\u001b[21A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[21A\u001b[2K\u001b[19A\u001b[2K\u001b[21A\u001b[2K\u001b[20A\u001b[2K\u001b[21A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[20A\u001b[2K\u001b[21A\u001b[2K\u001b[19A\u001b[2K\u001b[21A\u001b[2K\u001b[19A\u001b[2K\u001b[21A\u001b[2K\u001b[19A\u001b[2K\u001b[21A\u001b[2K\u001b[19A\u001b[2K\u001b[21A\u001b[2K\u001b[19A\u001b[2K\u001b[21A\u001b[2K\u001b[19A\u001b[2K\u001b[21A\u001b[2K\u001b[19A\u001b[2K\u001b[21A\u001b[2K\u001b[20A\u001b[2K\u001b[21A\u001b[2K\u001b[19A\u001b[2K\u001b[20A\u001b[2K\u001b[19A\u001b[2K\u001b[20A\u001b[2K\u001b[19A\u001b[2K\u001b[20A\u001b[2K\u001b[19A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[21A\u001b[2K\u001b[19A\u001b[2K\u001b[21A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[17A\u001b[2K\u001b[19A\u001b[2K\u001b[17A\u001b[2K\u001b[19A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[21A\u001b[2K\u001b[17A\u001b[2K\u001b[21A\u001b[2K\u001b[17A\u001b[2K\u001b[21A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[21A\u001b[2K\u001b[15A\u001b[2K\u001b[21A\u001b[2K\u001b[14A\u001b[2K\u001b[16A\u001b[2K\u001b[14A\u001b[2K\u001b[21A\u001b[2K\u001b[14A\u001b[2K\u001b[21A\u001b[2K\u001b[14A\u001b[2K\u001b[16A\u001b[2K\u001b[14A\u001b[2K\u001b[21A\u001b[2K\u001b[14A\u001b[2K\u001b[12A\u001b[2K\u001b[14A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[6A\u001b[2K\u001b[14A\u001b[2K\u001b[14A\u001b[2K\u001b[14A\u001b[2K\u001b[21A\u001b[2K\u001b[4A\u001b[2K\u001b[21A\u001b[2K\u001b[4A\u001b[2K\u001b[21A\u001b[2K\u001b[4A\u001b[2K\u001b[21A\u001b[2K\u001b[4A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[21A\u001b[2K\u001b[4A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[1A\u001b[2K\u001b[4A\u001b[2K\u001b[2A\u001b[2K\u001b[4A\u001b[2K\u001b[2A\u001b[2K\u001b[4A\u001b[2K\u001b[2A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[21A\u001b[2K\u001b[4A\u001b[2K\u001b[21A\u001b[2K\u001b[4A\u001b[2K\u001b[21A\u001b[2K\u001b[4A\u001b[2K\u001b[21A\u001b[2K\u001b[4A\u001b[2K\u001b[21A\u001b[2K\u001b[4A\u001b[2K\u001b[21A\u001b[2K\u001b[4A\u001b[2K\u001b[21A\u001b[2K\u001b[4A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[4A\u001b[2K\u001b[21A\u001b[2K\u001b[4A\u001b[2K\u001b[21A\u001b[2K\u001b[4A\u001b[2K\u001b[21A\u001b[2K\u001b[4A\u001b[2K\u001b[21A\u001b[2K\u001b[4A\u001b[2K\u001b[21A\u001b[2K\u001b[4A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[4A\u001b[2K\u001b[21A\u001b[2K\u001b[4A\u001b[2K\u001b[21A\u001b[2K\u001b[4A\u001b[2K\u001b[21A\u001b[2K\u001b[4A\u001b[2K\u001b[21A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[21A\u001b[2K\u001b[4A\u001b[2K\u001b[21A\u001b[2K\u001b[4A\u001b[2K\u001b[21A\u001b[2K\u001b[4A\u001b[2K\u001b[21A\u001b[2K\u001b[4A\u001b[2K\u001b[21A\u001b[2K\u001b[4A\u001b[2K\u001b[21A\u001b[2K\u001b[4A\u001b[2K\u001b[21A\u001b[2K\u001b[4A\u001b[2K\u001b[21A\u001b[2K\u001b[4A\u001b[2K\u001b[21A\u001b[2K\u001b[4A\u001b[2K\u001b[21A\u001b[2K\u001b[4A\u001b[2K\u001b[21A\u001b[2K\u001b[4A\u001b[2K\u001b[21A\u001b[2K\u001b[4A\u001b[2K\u001b[21A\u001b[2K\u001b[4A\u001b[2K\u001b[21A\u001b[2K\u001b[4A\u001b[2K\u001b[21A\u001b[2K\u001b[4A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[21A\u001b[2K\u001b[4A\u001b[2K\u001b[21A\u001b[2K\u001b[4A\u001b[2K\u001b[21A\u001b[2K\u001b[4A\u001b[2K\u001b[21A\u001b[2K\u001b[4A\u001b[2K\u001b[21A\u001b[2K\u001b[4A\u001b[2K\u001b[21A\u001b[2K\u001b[4A\u001b[2K\u001b[21A\u001b[2K\u001b[4A\u001b[2K\u001b[21A\u001b[2K\u001b[4A\u001b[2K\u001b[21A\u001b[2K\u001b[4A\u001b[2K\u001b[21A\u001b[2K\u001b[4A\u001b[2K\u001b[21A\u001b[2K\u001b[4A\u001b[2K\u001b[21A\u001b[2K\u001b[4A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[21A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[21A\u001b[2K\u001b[4A\u001b[2K\u001b[21A\u001b[2K\u001b[4A\u001b[2K\u001b[21A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[4A\u001b[2K\u001b[21A\u001b[2K\u001b[4A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[21A\u001b[2K\u001b[4A\u001b[2K\u001b[21A\u001b[2K\u001b[4A\u001b[2K\u001b[21A\u001b[2K\u001b[4A\u001b[2K\u001b[21A\u001b[2K\u001b[4A\u001b[2K\u001b[21A\u001b[2K\u001b[4A\u001b[2K\u001b[21A\u001b[2K\u001b[4A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[21A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2KExtracting  318.6MB/1.058GB\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[18A\u001b[2K\u001b[18A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[15A\u001b[2K\u001b[15A\u001b[2K\u001b[15A\u001b[2K\u001b[14A\u001b[2K\u001b[14A\u001b[2K\u001b[14A\u001b[2K\u001b[14A\u001b[2K\u001b[14A\u001b[2K\u001b[14A\u001b[2K\u001b[14A\u001b[2K\u001b[14A\u001b[2K\u001b[14A\u001b[2K\u001b[14A\u001b[2K\u001b[14A\u001b[2K\u001b[14A\u001b[2K\u001b[14A\u001b[2K\u001b[14A\u001b[2K\u001b[14A\u001b[2K\u001b[14A\u001b[2K\u001b[14A\u001b[2K\u001b[14A\u001b[2K\u001b[14A\u001b[2K\u001b[14A\u001b[2K\u001b[14A\u001b[2K\u001b[14A\u001b[2K\u001b[14A\u001b[2K\u001b[14A\u001b[2K\u001b[14A\u001b[2K\u001b[14A\u001b[2K\u001b[14A\u001b[2K\u001b[14A\u001b[2K\u001b[14A\u001b[2K\u001b[14A\u001b[2K\u001b[14A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2KExtracting  1.345GB/1.559GB\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KDigest: sha256:f3c0bda80c58aa937883410d29194dd4eec8835d675091f3a8c8764fe1092358\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/tf2-gpu.2-4:latest\n",
      " ---> 4306f1450ab2\n",
      "Step 2/5 : RUN pip install pip install tf-models-official tensorflow-text\n",
      " ---> Running in 3de4efdf6c9c\n",
      "Requirement already satisfied: pip in /opt/conda/lib/python3.7/site-packages (21.0.1)\n",
      "Collecting install\n",
      "  Downloading install-1.3.4-py3-none-any.whl (3.1 kB)\n",
      "Collecting tf-models-official\n",
      "  Downloading tf_models_official-2.4.0-py2.py3-none-any.whl (1.1 MB)\n",
      "Collecting tensorflow-text\n",
      "  Downloading tensorflow_text-2.4.3-cp37-cp37m-manylinux1_x86_64.whl (3.4 MB)\n",
      "Requirement already satisfied: tensorflow<2.5,>=2.4.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-text) (2.4.1)\n",
      "Requirement already satisfied: tensorflow-hub>=0.8.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-text) (0.9.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text) (3.14.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text) (1.1.2)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text) (1.12.1)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text) (3.3.0)\n",
      "Collecting grpcio~=1.32.0\n",
      "  Downloading grpcio-1.32.0-cp37-cp37m-manylinux2014_x86_64.whl (3.8 MB)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text) (1.1.0)\n",
      "Requirement already satisfied: six~=1.15.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text) (1.15.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text) (1.12)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text) (0.2.0)\n",
      "Requirement already satisfied: gast==0.3.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text) (0.3.3)\n",
      "Requirement already satisfied: absl-py~=0.10 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text) (0.10.0)\n",
      "Requirement already satisfied: wheel~=0.35 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text) (0.36.2)\n",
      "Requirement already satisfied: tensorboard~=2.4 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text) (2.4.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text) (2.4.0)\n",
      "Requirement already satisfied: numpy~=1.19.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text) (1.19.5)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text) (3.7.4.3)\n",
      "Requirement already satisfied: h5py~=2.10.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text) (2.10.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text) (1.8.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text) (1.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text) (2.25.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text) (3.3.3)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text) (1.26.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text) (53.0.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text) (4.7.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text) (4.2.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text) (3.4.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text) (0.4.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text) (1.26.3)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text) (2020.12.5)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text) (3.1.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from tf-models-official) (1.6.0)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2 MB)\n",
      "Collecting tensorflow-model-optimization>=0.4.1\n",
      "  Downloading tensorflow_model_optimization-0.5.0-py2.py3-none-any.whl (172 kB)\n",
      "Collecting tensorflow-addons\n",
      "  Downloading tensorflow_addons-0.12.1-cp37-cp37m-manylinux2010_x86_64.whl (703 kB)\n",
      "Requirement already satisfied: google-cloud-bigquery>=0.31.0 in /opt/conda/lib/python3.7/site-packages (from tf-models-official) (1.26.1)\n",
      "Collecting gin-config\n",
      "  Downloading gin_config-0.4.0-py2.py3-none-any.whl (46 kB)\n",
      "Collecting pycocotools\n",
      "  Downloading pycocotools-2.0.2.tar.gz (23 kB)\n",
      "Requirement already satisfied: google-api-python-client>=1.6.7 in /opt/conda/lib/python3.7/site-packages (from tf-models-official) (1.12.8)\n",
      "Collecting tf-slim>=1.1.0\n",
      "  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from tf-models-official) (3.3.4)\n",
      "Collecting seqeval\n",
      "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.7/site-packages (from tf-models-official) (8.1.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from tf-models-official) (5.4.1)\n",
      "Collecting Cython\n",
      "  Using cached Cython-0.29.22-cp37-cp37m-manylinux1_x86_64.whl (2.0 MB)\n",
      "Requirement already satisfied: psutil>=5.4.3 in /opt/conda/lib/python3.7/site-packages (from tf-models-official) (5.8.0)\n",
      "Requirement already satisfied: pandas>=0.22.0 in /opt/conda/lib/python3.7/site-packages (from tf-models-official) (1.2.2)\n",
      "Collecting py-cpuinfo>=3.3.0\n",
      "  Downloading py-cpuinfo-7.0.0.tar.gz (95 kB)\n",
      "Collecting opencv-python-headless\n",
      "  Downloading opencv_python_headless-4.5.1.48-cp37-cp37m-manylinux2014_x86_64.whl (37.6 MB)\n",
      "Collecting dataclasses\n",
      "  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: tensorflow-datasets in /opt/conda/lib/python3.7/site-packages (from tf-models-official) (3.0.0)\n",
      "Collecting kaggle>=1.3.9\n",
      "  Downloading kaggle-1.5.10.tar.gz (59 kB)\n",
      "Requirement already satisfied: oauth2client in /opt/conda/lib/python3.7/site-packages (from tf-models-official) (4.1.3)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.6.7->tf-models-official) (3.0.1)\n",
      "Requirement already satisfied: google-api-core<2dev,>=1.21.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.6.7->tf-models-official) (1.22.4)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.6.7->tf-models-official) (0.19.0)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.6.7->tf-models-official) (0.0.4)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official) (2021.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official) (1.52.0)\n",
      "Requirement already satisfied: google-resumable-media<2.0dev,>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery>=0.31.0->tf-models-official) (1.2.0)\n",
      "Requirement already satisfied: google-cloud-core<2.0dev,>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery>=0.31.0->tf-models-official) (1.3.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<2.0dev,>=0.5.0->google-cloud-bigquery>=0.31.0->tf-models-official) (1.1.2)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<2.0dev,>=0.5.0->google-cloud-bigquery>=0.31.0->tf-models-official) (1.14.5)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<2.0dev,>=0.5.0->google-cloud-bigquery>=0.31.0->tf-models-official) (2.20)\n",
      "Requirement already satisfied: pyparsing<3,>=2.4.2 in /opt/conda/lib/python3.7/site-packages (from httplib2<1dev,>=0.15.0->google-api-python-client>=1.6.7->tf-models-official) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.7/site-packages (from kaggle>=1.3.9->tf-models-official) (2.8.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from kaggle>=1.3.9->tf-models-official) (4.56.2)\n",
      "Requirement already satisfied: python-slugify in /opt/conda/lib/python3.7/site-packages (from kaggle>=1.3.9->tf-models-official) (4.0.1)\n",
      "Requirement already satisfied: dm-tree~=0.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow-model-optimization>=0.4.1->tf-models-official) (0.1.5)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text) (3.4.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->tf-models-official) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->tf-models-official) (0.10.0)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /opt/conda/lib/python3.7/site-packages (from python-slugify->kaggle>=1.3.9->tf-models-official) (1.3)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.7/site-packages (from seqeval->tf-models-official) (0.24.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official) (1.0.1)\n",
      "Collecting typeguard>=2.7\n",
      "  Downloading typeguard-2.11.1-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: promise in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets->tf-models-official) (2.3)\n",
      "Requirement already satisfied: tensorflow-metadata in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets->tf-models-official) (0.27.0)\n",
      "Requirement already satisfied: attrs>=18.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets->tf-models-official) (20.3.0)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets->tf-models-official) (0.3.1.1)\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets->tf-models-official) (0.18.2)\n",
      "Building wheels for collected packages: kaggle, py-cpuinfo, pycocotools, seqeval\n",
      "  Building wheel for kaggle (setup.py): started\n",
      "  Building wheel for kaggle (setup.py): finished with status 'done'\n",
      "  Created wheel for kaggle: filename=kaggle-1.5.10-py3-none-any.whl size=73268 sha256=ecded60cfada87a6f60948c1795fe8ff3e11fe46bda2b966deb9fe7ea23359a8\n",
      "  Stored in directory: /root/.cache/pip/wheels/ea/c5/fe/7e7fb5b3d1f150fac96188949b3d83d375a4c9df16ba557e52\n",
      "  Building wheel for py-cpuinfo (setup.py): started\n",
      "  Building wheel for py-cpuinfo (setup.py): finished with status 'done'\n",
      "  Created wheel for py-cpuinfo: filename=py_cpuinfo-7.0.0-py3-none-any.whl size=20070 sha256=2a36d2946e4f393785b1952e9b5615d6b6efec20f6a2c3c2f221f8002ae8f297\n",
      "  Stored in directory: /root/.cache/pip/wheels/d7/59/0d/58c5e576d9192261fa3da00466eebe6f7a1ac1873a7ab1f2ce\n",
      "  Building wheel for pycocotools (setup.py): started\n",
      "  Building wheel for pycocotools (setup.py): finished with status 'done'\n",
      "  Created wheel for pycocotools: filename=pycocotools-2.0.2-cp37-cp37m-linux_x86_64.whl size=272468 sha256=e89c85eb3237d028ed95defdf2e7eb4e8e8a9d87bbf6dcd80354482cd467bf82\n",
      "  Stored in directory: /root/.cache/pip/wheels/bc/cf/1b/e95c99c5f9d1648be3f500ca55e7ce55f24818b0f48336adaf\n",
      "  Building wheel for seqeval (setup.py): started\n",
      "  Building wheel for seqeval (setup.py): finished with status 'done'\n",
      "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16170 sha256=fd274208c4dca6f5c982a8d0afb6acec0324dd85638bc23626252841b8ba6ac4\n",
      "  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n",
      "Successfully built kaggle py-cpuinfo pycocotools seqeval\n",
      "Installing collected packages: grpcio, typeguard, Cython, tf-slim, tensorflow-model-optimization, tensorflow-addons, seqeval, sentencepiece, pycocotools, py-cpuinfo, opencv-python-headless, kaggle, gin-config, dataclasses, tf-models-official, tensorflow-text, install\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.35.0\n",
      "    Uninstalling grpcio-1.35.0:\n",
      "      Successfully uninstalled grpcio-1.35.0\n",
      "Successfully installed Cython-0.29.22 dataclasses-0.6 gin-config-0.4.0 grpcio-1.32.0 install-1.3.4 kaggle-1.5.10 opencv-python-headless-4.5.1.48 py-cpuinfo-7.0.0 pycocotools-2.0.2 sentencepiece-0.1.95 seqeval-1.2.2 tensorflow-addons-0.12.1 tensorflow-model-optimization-0.5.0 tensorflow-text-2.4.3 tf-models-official-2.4.0 tf-slim-1.1.0 typeguard-2.11.1\n",
      "\u001b[91mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tfx 0.27.0 requires kubernetes<12,>=10.0.1, but you have kubernetes 12.0.1 which is incompatible.\n",
      "tfx 0.27.0 requires pyarrow<3,>=1, but you have pyarrow 3.0.0 which is incompatible.\n",
      "tensorflow-transform 0.27.0 requires pyarrow<3,>=1, but you have pyarrow 3.0.0 which is incompatible.\n",
      "tensorflow-model-analysis 0.27.0 requires pyarrow<3,>=1, but you have pyarrow 3.0.0 which is incompatible.\n",
      "tensorflow-data-validation 0.27.0 requires joblib<0.15,>=0.12, but you have joblib 1.0.1 which is incompatible.\n",
      "tensorflow-data-validation 0.27.0 requires pyarrow<3,>=1, but you have pyarrow 3.0.0 which is incompatible.\n",
      "explainable-ai-sdk 1.1.0 requires numpy<1.19.0, but you have numpy 1.19.5 which is incompatible.\n",
      "apache-beam 2.27.0 requires httplib2<0.18.0,>=0.8, but you have httplib2 0.19.0 which is incompatible.\n",
      "apache-beam 2.27.0 requires pyarrow<3.0.0,>=0.15.1, but you have pyarrow 3.0.0 which is incompatible.\n",
      "\u001b[0mRemoving intermediate container 3de4efdf6c9c\n",
      " ---> 5d25da9418af\n",
      "Step 3/5 : WORKDIR /\n",
      " ---> Running in e1d8a6ba1836\n",
      "Removing intermediate container e1d8a6ba1836\n",
      " ---> 9cdf32963ce8\n",
      "Step 4/5 : COPY trainer /trainer\n",
      " ---> 7980a5c7e90e\n",
      "Step 5/5 : ENTRYPOINT [\"python\", \"-m\", \"trainer.task\"]\n",
      " ---> Running in 5db3966748a2\n",
      "Removing intermediate container 5db3966748a2\n",
      " ---> 68c5165c4492\n",
      "Successfully built 68c5165c4492\n",
      "Successfully tagged gcr.io/jk-demos/imdb_bert:latest\n"
     ]
    }
   ],
   "source": [
    "! docker build -t {TRAIN_IMAGE} ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The push refers to repository [gcr.io/jk-demos/imdb_bert]\n",
      "\n",
      "\u001b[1B923208c7: Preparing \n",
      "\u001b[1B2d9b52b0: Preparing \n",
      "\u001b[1B3b7141d8: Preparing \n",
      "\u001b[1B28f81e86: Preparing \n",
      "\u001b[1Bf77b619e: Preparing \n",
      "\u001b[1B22f25eab: Preparing \n",
      "\u001b[1B61cbfb7b: Preparing \n",
      "\u001b[1Bdb287c92: Preparing \n",
      "\u001b[1B0f881413: Preparing \n",
      "\u001b[1Be39f1882: Preparing \n",
      "\u001b[1B2059d805: Preparing \n",
      "\u001b[1Badd514e4: Preparing \n",
      "\u001b[1B40002f73: Preparing \n",
      "\u001b[1B7670164c: Preparing \n",
      "\u001b[1B88a169f3: Preparing \n",
      "\u001b[1Ba13b2926: Preparing \n",
      "\u001b[1Bd631abca: Preparing \n",
      "\u001b[1Bea8063f8: Preparing \n",
      "\u001b[1B5280894d: Preparing \n",
      "\u001b[1B65bc85a8: Preparing \n",
      "\u001b[1B00c31be3: Preparing \n",
      "\u001b[1B18b890fc: Preparing \n",
      "\u001b[1Ba7c9e3d1: Preparing \n",
      "\u001b[1B4dce1444: Preparing \n",
      "\u001b[1B30bcc944: Preparing \n",
      "\u001b[1Be116c0c0: Preparing \n",
      "\u001b[1B4df0ad6c: Preparing \n",
      "\u001b[1Bdf553184: Preparing \n",
      "\u001b[28Bd9b52b0: Pushed   199.5MB/197.3MB\u001b[28A\u001b[2K\u001b[26A\u001b[2K\u001b[28A\u001b[2K\u001b[20A\u001b[2K\u001b[18A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[12A\u001b[2K\u001b[10A\u001b[2K\u001b[7A\u001b[2K\u001b[5A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2K\u001b[28A\u001b[2Klatest: digest: sha256:b674ac0ecdc5c60c039eb0205e75e03a9f86aced9d972203f0ecd04e4086abdc size: 6408\n"
     ]
    }
   ],
   "source": [
    "! docker push {TRAIN_IMAGE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively you could use Cloud Build."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!gcloud builds submit --tag {TRAIN_IMAGE} ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the image locally\n",
    "\n",
    "It may be difficult to troubleshoot distributed training jobs running in AI Platform. You can perform some level of troubleshooting by simulating a distributed training environment on your AI Platform Notebooks instance.\n",
    "\n",
    "Let's assume that you have provisioned your instance with 4 GPUs. To simulate a distributed environment with two nodes, each equipped with two GPUs you can start two local containers configured as per below sample commands. Execute these commands from Jupyter terminal windows.\n",
    "\n",
    "```\n",
    "docker run --rm -it --gpus '\"device=0,1\"' \\\n",
    "--env TF_CONFIG='{\"cluster\": {\"worker\": [\"localhost:12345\", \"localhost:23456\"]}, \"task\": {\"type\": \"worker\", \"index\": 0} }' \\\n",
    "--network=host \\\n",
    "gcr.io/jk-demos/imdb_bert --epochs=2 --steps_per_epoch=20 --eval_steps=10 --auto_shard_policy=data --job_dir=gs://jk-demos-bucket/test_run\n",
    "```\n",
    "\n",
    "```\n",
    "docker run --rm -it --gpus '\"device=2,3\"' \\\n",
    "--env TF_CONFIG='{\"cluster\": {\"worker\": [\"localhost:12345\", \"localhost:23456\"]}, \"task\": {\"type\": \"worker\", \"index\": 1} }' \\\n",
    "--network=host \\\n",
    "gcr.io/jk-demos/imdb_bert --epochs=2 --steps_per_epoch=20 --eval_steps=10 --auto_shard_policy=data --job_dir=gs://jk-demos-bucket/test_run\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting training jobs\n",
    "\n",
    "The AI Platform (Unified) SDK works as a client/server model. On your side, the Python script, you  create a client that sends requests and receives responses from the server -- AI Platform. Requests and responses conform to the schemas documented in [AI Platform API Reference](https://cloud.google.com/ai-platform-unified/docs/reference/rest/v1/projects.locations.batchPredictionJobs/create).\n",
    "\n",
    "We will use the term specification to refer to a formatted request. To submit a Custom job request you need to create a Custom job specification.\n",
    "\n",
    "The custom job specification comprises two parts:\n",
    "- A worker pool configuration, and\n",
    "- A scheduling configuration\n",
    "\n",
    "For single-node training, you define a single worker pool. For multi-node distributed training, multiple worker pools are defined.\n",
    "\n",
    "Within a worker pool specification, you configure:\n",
    "- Machine types and accelerators\n",
    "- Configuration of what training code the worker pool runs. \n",
    "\n",
    "For jobs using custom containers (like in this sample), the latter section of a worker pool specification contains a custom container configuration, including the URI of the container image and parameters passed to the container.\n",
    "\n",
    "The scheduling configuration includes parameters related to queuing and scheduling of custom jobs, including the maximum job running time and the job restart policy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assembling the job specification for single-node training\n",
    "\n",
    "For this job we will use a single `n1-standard-4` machine with 2 NVidia V100 GPUs and the container image created in the previous sections of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MACHINE_TYPE = 'n1-standard-4'\n",
    "TRAIN_GPU, TRAIN_NGPU = (aip.AcceleratorType.NVIDIA_TESLA_V100, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When configuring a custom container you pass the command line parameters expected by your script through the `args` field of the container specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'display_name': 'imdb-bert-job-20210303221013',\n",
       " 'job_spec': {'worker_pool_specs': [{'replica_count': 1,\n",
       "    'machine_spec': {'machine_type': 'n1-standard-4',\n",
       "     'accelerator_type': <AcceleratorType.NVIDIA_TESLA_V100: 3>,\n",
       "     'accelerator_count': 2},\n",
       "    'container_spec': {'image_uri': 'gcr.io/jk-demos/imdb_bert',\n",
       "     'args': ['--epochs=3',\n",
       "      '--steps_per_epoch=200',\n",
       "      '--eval_steps=50',\n",
       "      '--training_data_path=gs://jk-demos-bucket/tfrecords/train',\n",
       "      '--validation_data_path=gs://jk-demos-bucket/tfrecords/valid',\n",
       "      '--job_dir=gs://jk-demos-bucket/jobs/job-20210303221013',\n",
       "      '--per_replica_batch_size=32',\n",
       "      '--strategy=mirrored',\n",
       "      '--auto_shard_policy=data']}}]}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 3\n",
    "steps_per_epoch = 200\n",
    "eval_steps = 50\n",
    "per_replica_batch_size = 32\n",
    "training_data_path = 'gs://jk-demos-bucket/tfrecords/train'\n",
    "validation_data_path = 'gs://jk-demos-bucket/tfrecords/valid'\n",
    "job_id = 'job-{}'.format(datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
    "job_dir = 'gs://jk-demos-bucket/jobs/{}'.format(job_id)\n",
    "\n",
    "worker_pool_spec = [\n",
    "    {\n",
    "        \"replica_count\": 1,\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": MACHINE_TYPE,\n",
    "            \"accelerator_type\": TRAIN_GPU,\n",
    "            \"accelerator_count\": TRAIN_NGPU\n",
    "        },\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": TRAIN_IMAGE,\n",
    "            \"args\": [\n",
    "                \"--epochs=\" + str(epochs),\n",
    "                \"--steps_per_epoch=\" + str(steps_per_epoch),\n",
    "                \"--eval_steps=\" + str(eval_steps),\n",
    "                \"--training_data_path=\" + training_data_path,\n",
    "                \"--validation_data_path=\" + validation_data_path,\n",
    "                \"--job_dir=\" + job_dir,\n",
    "                \"--per_replica_batch_size=\" + str(per_replica_batch_size),\n",
    "                \"--strategy=mirrored\",\n",
    "                \"--auto_shard_policy=data\",\n",
    "            ]\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "custom_job = {\n",
    "        \"display_name\": f'imdb-bert-{job_id}',\n",
    "        \"job_spec\": {\n",
    "            \"worker_pool_specs\": worker_pool_spec\n",
    "        },\n",
    "    }\n",
    "\n",
    "custom_job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting the job\n",
    "\n",
    "To submit the job you need to create a Job Service client and invoke the `create_custom_job` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name: \"projects/993115309906/locations/us-central1/customJobs/2556364534579200000\"\n",
       "display_name: \"imdb-bert-job-20210303221013\"\n",
       "job_spec {\n",
       "  worker_pool_specs {\n",
       "    machine_spec {\n",
       "      machine_type: \"n1-standard-4\"\n",
       "      accelerator_type: NVIDIA_TESLA_V100\n",
       "      accelerator_count: 2\n",
       "    }\n",
       "    replica_count: 1\n",
       "    disk_spec {\n",
       "      boot_disk_type: \"pd-ssd\"\n",
       "      boot_disk_size_gb: 100\n",
       "    }\n",
       "    container_spec {\n",
       "      image_uri: \"gcr.io/jk-demos/imdb_bert\"\n",
       "      args: \"--epochs=3\"\n",
       "      args: \"--steps_per_epoch=200\"\n",
       "      args: \"--eval_steps=50\"\n",
       "      args: \"--training_data_path=gs://jk-demos-bucket/tfrecords/train\"\n",
       "      args: \"--validation_data_path=gs://jk-demos-bucket/tfrecords/valid\"\n",
       "      args: \"--job_dir=gs://jk-demos-bucket/jobs/job-20210303221013\"\n",
       "      args: \"--per_replica_batch_size=32\"\n",
       "      args: \"--strategy=mirrored\"\n",
       "      args: \"--auto_shard_policy=data\"\n",
       "    }\n",
       "  }\n",
       "}\n",
       "state: JOB_STATE_PENDING\n",
       "create_time {\n",
       "  seconds: 1614809668\n",
       "  nanos: 632906000\n",
       "}\n",
       "update_time {\n",
       "  seconds: 1614809668\n",
       "  nanos: 632906000\n",
       "}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client_options = {\"api_endpoint\": API_ENDPOINT}\n",
    "\n",
    "client = aip.JobServiceClient(client_options=client_options)\n",
    "response = client.create_custom_job(parent=PARENT, custom_job=custom_job)\n",
    "job_name = response.name\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monitoring the job\n",
    "\n",
    "You can monitor the job through GCP Console or programmaticaly by using the `client.get_custom_job()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name: \"projects/993115309906/locations/us-central1/customJobs/2556364534579200000\"\n",
       "display_name: \"imdb-bert-job-20210303221013\"\n",
       "job_spec {\n",
       "  worker_pool_specs {\n",
       "    machine_spec {\n",
       "      machine_type: \"n1-standard-4\"\n",
       "      accelerator_type: NVIDIA_TESLA_V100\n",
       "      accelerator_count: 2\n",
       "    }\n",
       "    replica_count: 1\n",
       "    disk_spec {\n",
       "      boot_disk_type: \"pd-ssd\"\n",
       "      boot_disk_size_gb: 100\n",
       "    }\n",
       "    container_spec {\n",
       "      image_uri: \"gcr.io/jk-demos/imdb_bert\"\n",
       "      args: \"--epochs=3\"\n",
       "      args: \"--steps_per_epoch=200\"\n",
       "      args: \"--eval_steps=50\"\n",
       "      args: \"--training_data_path=gs://jk-demos-bucket/tfrecords/train\"\n",
       "      args: \"--validation_data_path=gs://jk-demos-bucket/tfrecords/valid\"\n",
       "      args: \"--job_dir=gs://jk-demos-bucket/jobs/job-20210303221013\"\n",
       "      args: \"--per_replica_batch_size=32\"\n",
       "      args: \"--strategy=mirrored\"\n",
       "      args: \"--auto_shard_policy=data\"\n",
       "    }\n",
       "  }\n",
       "}\n",
       "state: JOB_STATE_PENDING\n",
       "create_time {\n",
       "  seconds: 1614809668\n",
       "  nanos: 632906000\n",
       "}\n",
       "start_time {\n",
       "  seconds: 1614809668\n",
       "  nanos: 854581000\n",
       "}\n",
       "update_time {\n",
       "  seconds: 1614809672\n",
       "  nanos: 533838000\n",
       "}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = client.get_custom_job(name=job_name)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assembling the job specification for multi-worker training\n",
    "\n",
    "If you run a distributed training job with AI Platform, you specify multiple machines (nodes) in a training cluster. The training service allocates the resources for the machine types you specify. Your running job on a given node is called a replica.\n",
    "\n",
    "Each replica in the training cluster is given a single role or task in distributed training. For example:\n",
    "\n",
    "- Primary replica: Exactly one replica is designated the primary replica. This task manages the others and reports status for the job as a whole.\n",
    "- Worker(s): One or more replicas may be designated as workers. These replicas do their portion of the work as you designate in your job configuration.\n",
    "- Parameter server(s): If supported by your ML framework, one or more replicas may be designated as parameter servers. These replicas store model parameters and coordinate shared model state between the workers.\n",
    "- Evaluator(s): If supported by your ML framework, one or more replicas may be designated as evaluators. These replicas can be used to evaluate your model. If you are using TensorFlow, note that TensorFlow generally expects that you use no more than one evaluator.\n",
    "\n",
    "You configure the role by mapping to a worker pool specification:\n",
    "\n",
    "- First worker pool specification (index 0 in the `workerPoolSpecs` list) maps to Primary or chief worker. There can be only one replica configured in the first worker pool specification\n",
    "- Second worker pool specification maps to secondary workers\n",
    "- Third worker pool specification maps to parameters servers, and\n",
    "- Fourth worker pool specification maps to evaluators\n",
    "\n",
    "Our second job will be a multi-worker distributed training job with one chief and one secondary worker. Both replicas will run on `n1-standard-4` machines with two NVidia V100 GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "MACHINE_TYPE = 'n1-standard-4'\n",
    "TRAIN_GPU, TRAIN_NGPU = (aip.AcceleratorType.NVIDIA_TESLA_V100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'display_name': 'imdb-bert-job-20210303223833',\n",
       " 'job_spec': {'worker_pool_specs': [{'replica_count': 1,\n",
       "    'machine_spec': {'machine_type': 'n1-standard-4',\n",
       "     'accelerator_type': <AcceleratorType.NVIDIA_TESLA_V100: 3>,\n",
       "     'accelerator_count': 2},\n",
       "    'container_spec': {'image_uri': 'gcr.io/jk-demos/imdb_bert',\n",
       "     'args': ['--epochs=100',\n",
       "      '--steps_per_epoch=500',\n",
       "      '--eval_steps=100',\n",
       "      '--per_replica_batch_size=32',\n",
       "      '--training_data_path=gs://jk-demos-bucket/tfrecords/train',\n",
       "      '--validation_data_path=gs://jk-demos-bucket/tfrecords/valid',\n",
       "      '--job_dir=gs://jk-demos-bucket/jobs/job-20210303223833',\n",
       "      '--strategy=multiworker',\n",
       "      '--auto_shard_policy=data']}},\n",
       "   {'replica_count': 1,\n",
       "    'machine_spec': {'machine_type': 'n1-standard-4',\n",
       "     'accelerator_type': <AcceleratorType.NVIDIA_TESLA_V100: 3>,\n",
       "     'accelerator_count': 2},\n",
       "    'container_spec': {'image_uri': 'gcr.io/jk-demos/imdb_bert',\n",
       "     'args': ['--epochs=100',\n",
       "      '--steps_per_epoch=500',\n",
       "      '--eval_steps=100',\n",
       "      '--per_replica_batch_size=32',\n",
       "      '--training_data_path=gs://jk-demos-bucket/tfrecords/train',\n",
       "      '--validation_data_path=gs://jk-demos-bucket/tfrecords/valid',\n",
       "      '--job_dir=gs://jk-demos-bucket/jobs/job-20210303223833',\n",
       "      '--strategy=multiworker',\n",
       "      '--auto_shard_policy=data']}}]}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 100\n",
    "steps_per_epoch = 500\n",
    "eval_steps = 100\n",
    "training_data_path = 'gs://jk-demos-bucket/tfrecords/train'\n",
    "validation_data_path = 'gs://jk-demos-bucket/tfrecords/valid'\n",
    "job_id = 'job-{}'.format(datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
    "job_dir = 'gs://jk-demos-bucket/jobs/{}'.format(job_id)\n",
    "\n",
    "worker_pool_spec = [\n",
    "    {\n",
    "        \"replica_count\": 1,\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": MACHINE_TYPE,\n",
    "            \"accelerator_type\": TRAIN_GPU,\n",
    "            \"accelerator_count\": TRAIN_NGPU\n",
    "        },\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": TRAIN_IMAGE,\n",
    "            \"args\": [\n",
    "                \"--epochs=\" + str(epochs),\n",
    "                \"--steps_per_epoch=\" + str(steps_per_epoch),\n",
    "                \"--eval_steps=\" + str(eval_steps),\n",
    "                \"--per_replica_batch_size=\" + str(per_replica_batch_size),\n",
    "                \"--training_data_path=\" + training_data_path,\n",
    "                \"--validation_data_path=\" + validation_data_path,\n",
    "                \"--job_dir=\" + job_dir,\n",
    "                \"--strategy=multiworker\",\n",
    "                \"--auto_shard_policy=data\",\n",
    "            ]\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"replica_count\": 1,\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": MACHINE_TYPE,\n",
    "            \"accelerator_type\": TRAIN_GPU,\n",
    "            \"accelerator_count\": TRAIN_NGPU\n",
    "        },\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": TRAIN_IMAGE,\n",
    "            \"args\": [\n",
    "                \"--epochs=\" + str(epochs),\n",
    "                \"--steps_per_epoch=\" + str(steps_per_epoch),\n",
    "                \"--eval_steps=\" + str(eval_steps),\n",
    "                \"--per_replica_batch_size=\" + str(per_replica_batch_size),\n",
    "                \"--training_data_path=\" + training_data_path,\n",
    "                \"--validation_data_path=\" + validation_data_path,\n",
    "                \"--job_dir=\" + job_dir,\n",
    "                \"--strategy=multiworker\",\n",
    "                \"--auto_shard_policy=data\",\n",
    "            ]\n",
    "        },\n",
    "    },\n",
    "]\n",
    "\n",
    "custom_job = {\n",
    "        \"display_name\": f'imdb-bert-{job_id}',\n",
    "        \"job_spec\": {\n",
    "            \"worker_pool_specs\": worker_pool_spec\n",
    "        },\n",
    "    }\n",
    "\n",
    "custom_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name: \"projects/993115309906/locations/us-central1/customJobs/3254422476821626880\"\n",
       "display_name: \"imdb-bert-job-20210303223833\"\n",
       "job_spec {\n",
       "  worker_pool_specs {\n",
       "    machine_spec {\n",
       "      machine_type: \"n1-standard-4\"\n",
       "      accelerator_type: NVIDIA_TESLA_V100\n",
       "      accelerator_count: 2\n",
       "    }\n",
       "    replica_count: 1\n",
       "    disk_spec {\n",
       "      boot_disk_type: \"pd-ssd\"\n",
       "      boot_disk_size_gb: 100\n",
       "    }\n",
       "    container_spec {\n",
       "      image_uri: \"gcr.io/jk-demos/imdb_bert\"\n",
       "      args: \"--epochs=100\"\n",
       "      args: \"--steps_per_epoch=500\"\n",
       "      args: \"--eval_steps=100\"\n",
       "      args: \"--per_replica_batch_size=32\"\n",
       "      args: \"--training_data_path=gs://jk-demos-bucket/tfrecords/train\"\n",
       "      args: \"--validation_data_path=gs://jk-demos-bucket/tfrecords/valid\"\n",
       "      args: \"--job_dir=gs://jk-demos-bucket/jobs/job-20210303223833\"\n",
       "      args: \"--strategy=multiworker\"\n",
       "      args: \"--auto_shard_policy=data\"\n",
       "    }\n",
       "  }\n",
       "  worker_pool_specs {\n",
       "    machine_spec {\n",
       "      machine_type: \"n1-standard-4\"\n",
       "      accelerator_type: NVIDIA_TESLA_V100\n",
       "      accelerator_count: 2\n",
       "    }\n",
       "    replica_count: 1\n",
       "    disk_spec {\n",
       "      boot_disk_type: \"pd-ssd\"\n",
       "      boot_disk_size_gb: 100\n",
       "    }\n",
       "    container_spec {\n",
       "      image_uri: \"gcr.io/jk-demos/imdb_bert\"\n",
       "      args: \"--epochs=100\"\n",
       "      args: \"--steps_per_epoch=500\"\n",
       "      args: \"--eval_steps=100\"\n",
       "      args: \"--per_replica_batch_size=32\"\n",
       "      args: \"--training_data_path=gs://jk-demos-bucket/tfrecords/train\"\n",
       "      args: \"--validation_data_path=gs://jk-demos-bucket/tfrecords/valid\"\n",
       "      args: \"--job_dir=gs://jk-demos-bucket/jobs/job-20210303223833\"\n",
       "      args: \"--strategy=multiworker\"\n",
       "      args: \"--auto_shard_policy=data\"\n",
       "    }\n",
       "  }\n",
       "}\n",
       "state: JOB_STATE_PENDING\n",
       "create_time {\n",
       "  seconds: 1614811118\n",
       "  nanos: 900724000\n",
       "}\n",
       "update_time {\n",
       "  seconds: 1614811118\n",
       "  nanos: 900724000\n",
       "}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client_options = {\"api_endpoint\": API_ENDPOINT}\n",
    "\n",
    "client = aip.JobServiceClient(client_options=client_options)\n",
    "response = client.create_custom_job(parent=PARENT, custom_job=custom_job)\n",
    "job_name = response.name\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name: \"projects/993115309906/locations/us-central1/customJobs/3254422476821626880\"\n",
       "display_name: \"imdb-bert-job-20210303223833\"\n",
       "job_spec {\n",
       "  worker_pool_specs {\n",
       "    machine_spec {\n",
       "      machine_type: \"n1-standard-4\"\n",
       "      accelerator_type: NVIDIA_TESLA_V100\n",
       "      accelerator_count: 2\n",
       "    }\n",
       "    replica_count: 1\n",
       "    disk_spec {\n",
       "      boot_disk_type: \"pd-ssd\"\n",
       "      boot_disk_size_gb: 100\n",
       "    }\n",
       "    container_spec {\n",
       "      image_uri: \"gcr.io/jk-demos/imdb_bert\"\n",
       "      args: \"--epochs=100\"\n",
       "      args: \"--steps_per_epoch=500\"\n",
       "      args: \"--eval_steps=100\"\n",
       "      args: \"--per_replica_batch_size=32\"\n",
       "      args: \"--training_data_path=gs://jk-demos-bucket/tfrecords/train\"\n",
       "      args: \"--validation_data_path=gs://jk-demos-bucket/tfrecords/valid\"\n",
       "      args: \"--job_dir=gs://jk-demos-bucket/jobs/job-20210303223833\"\n",
       "      args: \"--strategy=multiworker\"\n",
       "      args: \"--auto_shard_policy=data\"\n",
       "    }\n",
       "  }\n",
       "  worker_pool_specs {\n",
       "    machine_spec {\n",
       "      machine_type: \"n1-standard-4\"\n",
       "      accelerator_type: NVIDIA_TESLA_V100\n",
       "      accelerator_count: 2\n",
       "    }\n",
       "    replica_count: 1\n",
       "    disk_spec {\n",
       "      boot_disk_type: \"pd-ssd\"\n",
       "      boot_disk_size_gb: 100\n",
       "    }\n",
       "    container_spec {\n",
       "      image_uri: \"gcr.io/jk-demos/imdb_bert\"\n",
       "      args: \"--epochs=100\"\n",
       "      args: \"--steps_per_epoch=500\"\n",
       "      args: \"--eval_steps=100\"\n",
       "      args: \"--per_replica_batch_size=32\"\n",
       "      args: \"--training_data_path=gs://jk-demos-bucket/tfrecords/train\"\n",
       "      args: \"--validation_data_path=gs://jk-demos-bucket/tfrecords/valid\"\n",
       "      args: \"--job_dir=gs://jk-demos-bucket/jobs/job-20210303223833\"\n",
       "      args: \"--strategy=multiworker\"\n",
       "      args: \"--auto_shard_policy=data\"\n",
       "    }\n",
       "  }\n",
       "}\n",
       "state: JOB_STATE_PENDING\n",
       "create_time {\n",
       "  seconds: 1614811118\n",
       "  nanos: 900724000\n",
       "}\n",
       "start_time {\n",
       "  seconds: 1614811119\n",
       "  nanos: 140956000\n",
       "}\n",
       "update_time {\n",
       "  seconds: 1614811119\n",
       "  nanos: 455801000\n",
       "}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_name = response.name\n",
    "\n",
    "response = client.get_custom_job(name=job_name)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ucaip_customjob_image_container.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "name": "tf2-gpu.2-4.mnightly-2021-02-12-debian-10-test",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:mnightly-2021-02-12-debian-10-test"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
