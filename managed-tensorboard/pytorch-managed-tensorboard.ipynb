{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2020 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitoring PyTorch experiments with AI Platform TensorBoard\n",
    "\n",
    "This code sample demonstrates how to configure AI Platform TensorBoard (Experimental) to monitor PyTorch training jobs.\n",
    "\n",
    "AI Platform Tensorboard is an enterprise ready, managed version of TensorBoard, a Google Open Source project for Machine Learning experiment visualization, that is tightly integrated with the Google Cloud AI Platform.\n",
    "\n",
    "TensorBoard provides the visualization and tooling needed for machine learning experimentation:\n",
    "* Tracking and visualizing metrics such as loss and accuracy\n",
    "* Visualizing the model graph (ops and layers)\n",
    "* Viewing histograms of weights, biases, or other tensors as they change over time\n",
    "* Projecting embeddings to a lower dimensional space\n",
    "* Displaying images, text, and audio data\n",
    "* Profiling TensorFlow programs\n",
    "* And much more\n",
    "\n",
    "Note that currently, AI Platform TensorBoard requires AI Platform Training and only supports the Scalars dashboard. As support for other features of TensorBoard are added this notebook will be updated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites\n",
    "\n",
    "This notebook was designed to run on [AI Platform Notebooks](https://cloud.google.com/ai-platform/notebooks/docs) using the standard PyTorch 1.6+ image. Your notebook instance should be in the same project as the AI Platform TensorBoard and Training services.\n",
    "\n",
    "While AI Platform TensorBoard is in the Experimental stage your project must be allow-listed before using the service. Use the [signup form](https://docs.google.com/forms/d/e/1FAIpQLSfbvZ5xrFStX54qEUlJ6A0tWZ-O20i_t-Hifm0JvbX8do5IcQ/viewform) to request the acccess.\n",
    "\n",
    "After your project has been allow-listed make sure to [enable Cloud AI Platform API](https://console.cloud.google.com/apis/library/aiplatform.googleapis.com?q=cloud%20ai%20platform%20api&id=6189b0c0-23b1-46a4-a32f-70639e83fe9b)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario\n",
    "\n",
    "You will use AI Platform TensorBoard to monitor PyTorch AI Platform Training jobs. The training scenario is transfer learning for an image classification problem, inspired by the Kaggle's classic Dogs vs. Cats competition. \n",
    "\n",
    "You will run and monitor two types of AI Platform Training jobs: a custom training job and a hyperparameter tuning job. Both types of jobs will utilize the same custom docker image that encapsulates the training application and the PyTorch runtime.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import google.auth\n",
    "\n",
    "from google.auth.credentials import Credentials\n",
    "from google.auth.transport.requests import AuthorizedSession\n",
    "\n",
    "from typing import List, Optional, Text, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create AI Platform Training service account\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To integrate with AI Platform TensorBoard, AI Platform Training jobs have to run in the context a service account that has permisions to write logs to GCS and access the AI Platform TensorBoard service. \n",
    "\n",
    "If you don't have one already set up, create and configure a new service account using the following instructions. Note that by default, your AI Platform Notebook instance is running under the **Compute Engine default service account** that does not have the required permissions (specifically `iam.serviceAccounts.setIamPolicy`) to configure the service account. If this is the case, use a different environment  (for example **Cloud Shell**) and the account with the required permissions to run the below commands.\n",
    "\n",
    "\n",
    "1. Create a service account\n",
    "\n",
    "```\n",
    "PROJECT_ID=[YOUR_PROJECT_ID]\n",
    "USER_SA_NAME=[YOUR_SA_ACCOUNT_NAME]\n",
    "\n",
    "gcloud --project=$PROJECT_ID iam service-accounts create $USER_SA_NAME\n",
    "\n",
    "```\n",
    "\n",
    "2. Retrieve the internal service account used by AI Platform \n",
    "\n",
    "```\n",
    "GOOGLE_SA=$(gcloud projects get-iam-policy $PROJECT_ID \\\n",
    "    --flatten=\"bindings[].members\" --format=\"table(bindings.members)\" \\\n",
    "    --filter=\"bindings.role:roles/aiplatform.customCodeServiceAgent\" | \\\n",
    "    grep \"serviceAccount:\" | head -n1)\n",
    "```\n",
    "\n",
    "3. Give the AI Platform service account permissions to impersonate your service account\n",
    "\n",
    "```\n",
    "SA_EMAIL=\"${USER_SA_NAME}@${PROJECT_ID}.iam.gserviceaccount.com\"\n",
    "\n",
    "gcloud --project=$PROJECT_ID iam service-accounts add-iam-policy-binding \\\n",
    "    --role roles/iam.serviceAccountAdmin \\\n",
    "    --member $GOOGLE_SA $SA_EMAIL\n",
    "\n",
    "```\n",
    "\n",
    "4. Give your service account access to GCS and AI Platform Tensorboard service.\n",
    "\n",
    "```\n",
    "gcloud projects add-iam-policy-binding $PROJECT_ID \\\n",
    "    --member=\"serviceAccount:${SA_EMAIL}\" \\\n",
    "    --role=\"roles/storage.admin\"\n",
    "\n",
    "gcloud projects add-iam-policy-binding $PROJECT_ID \\\n",
    "    --member=\"serviceAccount:${SA_EMAIL}\" \\\n",
    "    --role=\"roles/aiplatform.user\"\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "Set the `SA_EMAIL` constant to the email account of your service account. If you followed the above steps to create the account you can display it by:\n",
    "\n",
    "```\n",
    "echo $SA_EMAIL\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "SA_EMAIL = 'aip-training@jk-mlops-dev.iam.gserviceaccount.com'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set AI Platform (Unified) constants\n",
    "\n",
    "`PROJECT_ID` - The GCP Project ID. Both your AI Platform Notebook instance and AI Platform Training jobs should be in the same project. Make sure to modify the placeholder with your Project ID.\n",
    "\n",
    "`CAIP_REGION` - A GCP compute region to use for the cloud services used in this notebook. Make sure to choose a region where [Cloud AI Platform services](https://cloud.google.com/ml-engine/docs/tensorflow/regions) are available. The default region is `us-central1`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'jk-mlops-dev'\n",
    "CAIP_REGION = 'us-central1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current version of AI Platform (Unified) Python SDK does not support the AI Platform TensorBoard service nor the AI Platform Training service intergration with AI Platform TensorBoard. To mitigate, the notebook calls the APIs directly using the REST interface. You don't need to change the below constants that define the API's endpoint and root resource paths. \n",
    "\n",
    "`CAIP_ENDPOINT` - The AI Platform (Unified) API service endpoint.\n",
    "\n",
    "`CAIP_PARENT_ALPHA` - The AI Platform (Unified) Alpha (Experimental) API root resource path. AI Platform (Unified) TensorBoard is in the Experimental stage.\n",
    "\n",
    "`CAIP_PARENT_BETA` - The AI Platform (Unified) Beta (Preview) API root resource path. AI Platform (Unified) Training is in the Preview stage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAIP_ENDPOINT = f'{CAIP_REGION}-aiplatform.googleapis.com'\n",
    "CAIP_PARENT_ALPHA = f'https://{CAIP_ENDPOINT}/v1alpha1/projects/{PROJECT_ID}/locations/{CAIP_REGION}'\n",
    "CAIP_PARENT_BETA = f'https://{CAIP_ENDPOINT}/v1beta1/projects/{PROJECT_ID}/locations/{CAIP_REGION}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a GCS bucket to store TensorBoard logs\n",
    "\n",
    "The training script writes TensorBoard logs to a GCS bucket from which the AI Platform Training service  ingests them to the TensorBoard service. The GCS bucket should be a regional bucket  in the same region where the training job is executed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://jk-mlops-dev-tensorboard-logs/...\n",
      "ServiceException: 409 Bucket jk-mlops-dev-tensorboard-logs already exists.\n"
     ]
    }
   ],
   "source": [
    "GCS_BUCKET_NAME = f'{PROJECT_ID}-tensorboard-logs'\n",
    "\n",
    "!gsutil mb -l {CAIP_REGION} gs://{GCS_BUCKET_NAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a TensorBoard instance\n",
    "\n",
    "You will now create an AI Platform TensorBoard instance. As noted before you will the REST interface to invoke the APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create an authorized session "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials, _ = google.auth.default()\n",
    "authed_session = AuthorizedSession(credentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a TensorBoard resorce\n",
    "\n",
    "The following REST call creates a TensorBoard resource and sets its display name. Note that multiple resources can share the same display name, so each execution of the following cell creates a new TensorBoard instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'projects/895222332033/locations/us-central1/tensorboards/7266276523785584640/operations/7568055082214752256',\n",
       " 'metadata': {'@type': 'type.googleapis.com/google.cloud.aiplatform.v1alpha1.CreateTensorboardOperationMetadata',\n",
       "  'genericMetadata': {'createTime': '2020-12-02T23:18:20.643780Z',\n",
       "   'updateTime': '2020-12-02T23:18:20.643780Z'}}}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorboard_display_name = 'pytorch_tensorboard'\n",
    "api_url = f'{CAIP_PARENT_ALPHA}/tensorboards'\n",
    "\n",
    "request_body = {\n",
    "    'display_name': tensorboard_display_name\n",
    "}\n",
    "\n",
    "response = authed_session.post(api_url, data=json.dumps(request_body))\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List TensorBoard instances \n",
    "\n",
    "To verify that the instance was succesfully created list all instances with the set display name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tensorboards': [{'name': 'projects/895222332033/locations/us-central1/tensorboards/7266276523785584640',\n",
       "   'displayName': 'pytorch_tensorboard',\n",
       "   'createTime': '2020-12-02T23:18:20.643780Z',\n",
       "   'updateTime': '2020-12-02T23:18:20.736959Z',\n",
       "   'etag': 'AMEw9yPK-DNWAfpIRfcdqxlcQtrHYxvsYyiIDg2JUARjIKR2M5-DefMrXtUdr34lmsMl'}]}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_url = f'{CAIP_PARENT_ALPHA}/tensorboards?filter=display_name={tensorboard_display_name}'\n",
    "\n",
    "response = authed_session.get(api_url)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve a full name of your instance\n",
    "\n",
    "You can retrieve the fullname of the created instance from the JSON response. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'projects/895222332033/locations/us-central1/tensorboards/7266276523785584640'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorboard_id = response.json()['tensorboards'][0]['name']\n",
    "tensorboard_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your environment is ready. You will now prepare a custom PyTorch training container and submit and monitor AI Platform Training jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing a training container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train_eval.py\n",
    "\n",
    "# Copyright 2020 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#            http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "\n",
    "import argparse\n",
    "import hypertune\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "\n",
    "DEFAULT_ROOT = '/tmp'\n",
    "\n",
    "def get_catsanddogs(root):\n",
    "    \"\"\"\n",
    "    Creates training and validation Datasets based on images\n",
    "    of cats and dogs from \n",
    "    https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Download and extract the images\n",
    "    source_url = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'\n",
    "    local_filename = source_url.split('/')[-1]\n",
    "    datasets.utils.download_url(source_url, root, )\n",
    "    path_to_zip = os.path.join(root, local_filename)\n",
    "    with zipfile.ZipFile(path_to_zip, 'r') as zip_ref:\n",
    "        zip_ref.extractall(root)\n",
    "    \n",
    "    \n",
    "    # Create datasets\n",
    "    train_transforms = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(256),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.CenterCrop(size=224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "    \n",
    "    val_transforms = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "    \n",
    "    train_dataset = datasets.ImageFolder(\n",
    "        root=os.path.join(path_to_zip[:-4], 'train'),\n",
    "        transform=train_transforms)\n",
    "    \n",
    "    val_dataset = datasets.ImageFolder(\n",
    "        root=os.path.join(path_to_zip[:-4], 'validation'),\n",
    "        transform=val_transforms\n",
    "    )\n",
    "    \n",
    "    return train_dataset, val_dataset\n",
    "    \n",
    "\n",
    "\n",
    "def get_model(num_layers, dropout_ratio, num_classes):\n",
    "    \"\"\"\n",
    "    Creates a convolution net using ResNet50 trunk and\n",
    "    a custom head.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the ResNet50 trunk\n",
    "    model = models.resnet18(pretrained=True)\n",
    "\n",
    "    # Get the number of input features to the default head\n",
    "    num_features = model.fc.in_features\n",
    "\n",
    "    # Freeze trunk weights\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Define the new head\n",
    "    head = nn.Sequential(nn.Linear(num_features, num_layers),\n",
    "                         nn.ReLU(),\n",
    "                         nn.Dropout(dropout_ratio),\n",
    "                         nn.Linear(num_layers, num_classes))\n",
    "\n",
    "    # Replace the head\n",
    "    model.fc = head\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_eval(device, model, train_dataloader, valid_dataloader,\n",
    "               criterion, optimizer, scheduler, num_epochs, writer=None):\n",
    "    \"\"\"\n",
    "    Trains and evaluates a model.\n",
    "    \"\"\"\n",
    "    since = time.time()\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    hpt = hypertune.HyperTune()\n",
    "\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        num_train_examples = 0\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for inputs, labels in train_dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            num_train_examples += inputs.size(0)\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "        scheduler.step()\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        num_val_examples = 0\n",
    "        val_loss = 0\n",
    "        val_corrects = 0\n",
    "\n",
    "        for inputs, labels in valid_dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            num_val_examples += inputs.size(0)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            val_corrects += torch.sum(torch.eq(torch.max(outputs, 1)\n",
    "                                               [1], labels))\n",
    "\n",
    "        # Log epoch metrics\n",
    "        train_loss = train_loss / num_train_examples\n",
    "        val_loss = val_loss / num_val_examples\n",
    "        val_acc = val_corrects.double() / num_val_examples\n",
    "\n",
    "        print('Epoch: {}/{}, Training loss: {:.3f}, Validation loss: {:.3f}, Validation accuracy: {:.3f}'.format(\n",
    "              epoch, num_epochs, train_loss, val_loss, val_acc))\n",
    "\n",
    "        # Write to Tensorboard\n",
    "        if writer:\n",
    "            writer.add_scalars(\n",
    "                'Loss', {'training': train_loss, 'validation': val_loss}, epoch)\n",
    "            writer.add_scalar('Validation accuracy', val_acc, epoch)\n",
    "            writer.flush()\n",
    "            \n",
    "        # Report to HyperTune\n",
    "        hpt.report_hyperparameter_tuning_metric(\n",
    "            hyperparameter_metric_tag='accuracy',\n",
    "            metric_value=val_acc,\n",
    "            global_step=epoch\n",
    "        )\n",
    "\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, best_acc\n",
    "\n",
    "\n",
    "def get_args():\n",
    "    \"\"\"\n",
    "    Returns parsed command line arguments.\n",
    "    \"\"\"\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        '--num-epochs',\n",
    "        type=int,\n",
    "        default=20,\n",
    "        help='number of times to go through the data, default=20')\n",
    "    parser.add_argument(\n",
    "        '--batch-size',\n",
    "        default=32,\n",
    "        type=int,\n",
    "        help='number of records to read during each training step, default=32')\n",
    "    parser.add_argument(\n",
    "        '--num-layers',\n",
    "        default=64,\n",
    "        type=int,\n",
    "        help='number of hidden layers in the classification head , default=64')\n",
    "    parser.add_argument(\n",
    "        '--dropout-ratio',\n",
    "        default=0.5,\n",
    "        type=float,\n",
    "        help='dropout ration in the classification head , default=128')\n",
    "    parser.add_argument(\n",
    "        '--step-size',\n",
    "        default=7,\n",
    "        type=int,\n",
    "        help='step size of LR scheduler')\n",
    "    parser.add_argument(\n",
    "        '--log-dir',\n",
    "        type=str,\n",
    "        default='/tmp',\n",
    "        help='directory for TensorBoard logs')\n",
    "    parser.add_argument(\n",
    "        '--verbosity',\n",
    "        choices=['DEBUG', 'ERROR', 'FATAL', 'INFO', 'WARN'],\n",
    "        default='INFO')\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Parse command line arguments\n",
    "    args = get_args()\n",
    "    \n",
    "    # Create train and validation dataloaders\n",
    "    train_dataset, val_dataset = get_catsanddogs(DEFAULT_ROOT)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "    class_names = train_dataset.classes\n",
    "    \n",
    "    # Use GPU if available\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print('-' * 10)\n",
    "    print(f'Training on device: {device}')\n",
    "\n",
    "    # Configure training\n",
    "    model = get_model(args.num_layers, args.dropout_ratio, len(class_names))\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer, step_size=args.step_size, gamma=0.1)\n",
    "\n",
    "    # Set location for the TensorBoard logs\n",
    "    if 'AIP_TENSORBOARD_LOG_DIR' in os.environ:\n",
    "        log_dir = os.environ['AIP_TENSORBOARD_LOG_DIR']\n",
    "    else:\n",
    "        log_dir = args.log_dir\n",
    "\n",
    "    with SummaryWriter(log_dir) as writer:\n",
    "        trained_model, accuracy = train_eval(device, model, train_dataloader, val_dataloader,\n",
    "                                             criterion, optimizer, scheduler, args.num_epochs, writer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Dockerfile\n",
    "\n",
    "FROM gcr.io/deeplearning-platform-release/pytorch-gpu.1-6\n",
    "    \n",
    "RUN pip install -U tensorflow cloudml-hypertune\n",
    "\n",
    "ADD train_eval.py .\n",
    "\n",
    "ENTRYPOINT [\"python3\", \"train_eval.py\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_name = 'image_classifier'\n",
    "image_tag = 'latest'\n",
    "image_uri = f'gcr.io/{project_id}/{image_name}:{image_tag}'\n",
    "\n",
    "#!gcloud builds submit --tag {image_uri} ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting training jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit a custom training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = \"JOB_{}\".format(time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "base_output_dir = f'gs://{gcs_bucket_name}/{job_name}'\n",
    "tb_name = 'projects/895222332033/locations/us-central1/tensorboards/1989183660414205952'\n",
    "sa_email = 'aip-training@jk-mlops-dev.iam.gserviceaccount.com'\n",
    "\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_url = f'{beta_api_prefix}/customJobs'\n",
    "\n",
    "request_body = {\n",
    "    'display_name': job_name,\n",
    "    'job_spec': {\n",
    "        'worker_pool_specs': [\n",
    "            {\n",
    "                'replica_count': 1,\n",
    "                'machine_spec': {\n",
    "                    'machine_type': 'n1-standard-8',\n",
    "                    'accelerator_type': 'NVIDIA_TESLA_T4',\n",
    "                    'accelerator_count': 1\n",
    "                },\n",
    "                'container_spec': {\n",
    "                    'image_uri': image_uri,\n",
    "                    'args': [\n",
    "                        f'--num-epochs={num_epochs}'\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "        'base_output_directory': {\n",
    "            'output_uri_prefix': base_output_dir,\n",
    "        },\n",
    "        'service_account': sa_email,\n",
    "        'tensorboard': tb_name\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "response = authed_session.post(api_url, data=json.dumps(request_body))\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit a hyperparameter tuning job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_url = f'{beta_api_prefix}/hyperparameterTuningJobs'\n",
    "\n",
    "request_body = {\n",
    "    'display_name': job_name,\n",
    "    'study_spec' : {\n",
    "        'metrics': [\n",
    "            {\n",
    "                'metric_id': 'accuracy',\n",
    "                'goal': 'MAXIMIZE'\n",
    "            }\n",
    "        ],\n",
    "        'parameters': [\n",
    "            {\n",
    "                'parameter_id': 'batch-size',\n",
    "                'discrete_value_spec': {'values': [32, 64, 128]},\n",
    "                'scale_type': 'UNIT_LINEAR_SCALE'\n",
    "            },\n",
    "            {\n",
    "                'parameter_id': 'num-layers',\n",
    "                'discrete_value_spec': {'values': [64, 128]},\n",
    "                'scale_type': 'UNIT_LINEAR_SCALE'\n",
    "            }\n",
    "        ],\n",
    "    'algorithm':'GRID_SEARCH'\n",
    "    },\n",
    "    'maxTrialCount': 6,\n",
    "    'parallelTrialCount': 3,\n",
    "    'trial_job_spec': {\n",
    "        'worker_pool_specs': [\n",
    "            {\n",
    "                'replica_count': 1,\n",
    "                'machine_spec': {\n",
    "                    'machine_type': 'n1-standard-8',\n",
    "                    'accelerator_type': 'NVIDIA_TESLA_T4',\n",
    "                    'accelerator_count': 1\n",
    "                },\n",
    "                'container_spec': {\n",
    "                    'image_uri': image_uri,\n",
    "                    'args': [\n",
    "                        f'--num-epochs={num_epochs}'\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "        'base_output_directory': {\n",
    "            'output_uri_prefix': base_output_dir,\n",
    "        },\n",
    "        'service_account': sa_email,\n",
    "        'tensorboard': tb_name\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "response = authed_session.post(api_url, data=json.dumps(request_body))\n",
    "response.json()\n",
    "\n",
    "#request_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List all tensorboards in the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_url = f'{CAIP_PARENT_ALPHA}/tensorboards'\n",
    "\n",
    "response = authed_session.get(api_url)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete a TensorBoard resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'projects/895222332033/locations/us-central1/operations/3952790481343086592',\n",
       " 'metadata': {'@type': 'type.googleapis.com/google.cloud.aiplatform.v1alpha1.DeleteOperationMetadata',\n",
       "  'genericMetadata': {'createTime': '2020-12-02T23:18:08.357755Z',\n",
       "   'updateTime': '2020-12-02T23:18:08.357755Z'}},\n",
       " 'done': True,\n",
       " 'response': {'@type': 'type.googleapis.com/google.protobuf.Empty'}}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorboard_id = '6694319371109531648'\n",
    "\n",
    "api_url = f'{CAIP_PARENT_ALPHA}/tensorboards/{tensorboard_id}'\n",
    "\n",
    "response = authed_session.delete(api_url)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parking lot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize a few images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "\n",
    "# Get a batch of training data\n",
    "inputs, classes = next(trainiter)\n",
    "\n",
    "# Make a grid from batch\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "imshow(out, title=[class_names[x] for x in classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(num_layers, dropout_ratio, num_classes):\n",
    "    \"\"\"\n",
    "    Creates a convolution net using ResNet50 trunk and\n",
    "    a custom head.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the number of input features to the default head\n",
    "    model = models.resnet50(pretrained=True)\n",
    "    num_features = model.fc.in_features\n",
    "    \n",
    "    # Freeze trunk weights\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Define a new head\n",
    "    head = nn.Sequential(nn.Linear(num_features, num_layers),\n",
    "                         nn.ReLU(),\n",
    "                         nn.Dropout(dropout_ratio),\n",
    "                         nn.Linear(num_layers, num_classes))\n",
    "    \n",
    "    # Replace the head\n",
    "    model.fc = head\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 64\n",
    "dropout_ratio = 0.5\n",
    "num_classes = 2\n",
    "\n",
    "model = get_model(num_layers, dropout_ratio, num_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'{total_params:,} total parameters.')\n",
    "total_trainable_params = sum(\n",
    "    p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'{total_trainable_params:,} training parameters.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def train_model(device, dataloaders, dataset_sizes, model, criterion, \n",
    "                optimizer, scheduler, num_epochs=25, log_dir='/tmp'):\n",
    "    \n",
    "    since = time.time()\n",
    "    writer = SummaryWriter(log_dir)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    model = model.to(device)\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "            \n",
    "            # Write loss and accuracy to TensorBoard\n",
    "            writer.add_scalar(f'Loss/{phase}', epoch_loss, epoch)\n",
    "            writer.add_scalar(f'Acc/{phase}', epoch_acc, epoch)\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    writer.close()\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "\n",
    "\n",
    "data_dir = '/tmp/data/hymenoptera_data'\n",
    "batch_size = 32\n",
    "log_dir = 'gs://jk-tensorboards/experiments/102'\n",
    "num_epochs = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders, datasizes, class_names = get_dataloaders(data_dir, batch_size)\n",
    "\n",
    "model = train_model(device, dataloaders, datasizes, model, criterion, \n",
    "                    optimizer_ft, exp_lr_scheduler, num_epochs, log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorBoard setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "log_dir = 'gs://jk-tensorboards/experiments/1'\n",
    "\n",
    "writer = SummaryWriter(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# create grid of images\n",
    "img_grid = torchvision.utils.make_grid(images)\n",
    "\n",
    "# show images\n",
    "matplotlib_imshow(img_grid, one_channel=True)\n",
    "\n",
    "# write to tensorboard\n",
    "writer.add_image('four_fashion_mnist_images', img_grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.add_graph(net, images)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_n_random(data, labels, n=100):\n",
    "    '''\n",
    "    Selects n random datapoints and their corresponding labels from a dataset\n",
    "    '''\n",
    "    assert len(data) == len(labels)\n",
    "\n",
    "    perm = torch.randperm(len(data))\n",
    "    return data[perm][:n], labels[perm][:n]\n",
    "\n",
    "# select random images and their target indices\n",
    "images, labels = select_n_random(trainset.data, trainset.targets)\n",
    "\n",
    "# get the class labels for each image\n",
    "class_labels = [classes[lab] for lab in labels]\n",
    "\n",
    "# log embeddings\n",
    "features = images.view(-1, 28 * 28)\n",
    "writer.add_embedding(features,\n",
    "                    metadata=class_labels,\n",
    "                    label_img=images.unsqueeze(1))\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def images_to_probs(net, images):\n",
    "    '''\n",
    "    Generates predictions and corresponding probabilities from a trained\n",
    "    network and a list of images\n",
    "    '''\n",
    "    output = net(images)\n",
    "    # convert output probabilities to predicted class\n",
    "    _, preds_tensor = torch.max(output, 1)\n",
    "    preds = np.squeeze(preds_tensor.numpy())\n",
    "    return preds, [F.softmax(el, dim=0)[i].item() for i, el in zip(preds, output)]\n",
    "\n",
    "\n",
    "def plot_classes_preds(net, images, labels):\n",
    "    '''\n",
    "    Generates matplotlib Figure using a trained network, along with images\n",
    "    and labels from a batch, that shows the network's top prediction along\n",
    "    with its probability, alongside the actual label, coloring this\n",
    "    information based on whether the prediction was correct or not.\n",
    "    Uses the \"images_to_probs\" function.\n",
    "    '''\n",
    "    preds, probs = images_to_probs(net, images)\n",
    "    # plot the images in the batch, along with predicted and true labels\n",
    "    fig = plt.figure(figsize=(12, 48))\n",
    "    for idx in np.arange(4):\n",
    "        ax = fig.add_subplot(1, 4, idx+1, xticks=[], yticks=[])\n",
    "        matplotlib_imshow(images[idx], one_channel=True)\n",
    "        ax.set_title(\"{0}, {1:.1f}%\\n(label: {2})\".format(\n",
    "            classes[preds[idx]],\n",
    "            probs[idx] * 100.0,\n",
    "            classes[labels[idx]]),\n",
    "                    color=(\"green\" if preds[idx]==labels[idx].item() else \"red\"))\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_loss = 0.0\n",
    "for epoch in range(1):  # loop over the dataset multiple times\n",
    "\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:    # every 1000 mini-batches...\n",
    "\n",
    "            # ...log the running loss\n",
    "            writer.add_scalar('training loss',\n",
    "                            running_loss / 1000,\n",
    "                            epoch * len(trainloader) + i)\n",
    "\n",
    "            # ...log a Matplotlib Figure showing the model's predictions on a\n",
    "            # random mini-batch\n",
    "            writer.add_figure('predictions vs. actuals',\n",
    "                            plot_classes_preds(net, inputs, labels),\n",
    "                            global_step=epoch * len(trainloader) + i)\n",
    "            running_loss = 0.0\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. gets the probability predictions in a test_size x num_classes Tensor\n",
    "# 2. gets the preds in a test_size Tensor\n",
    "# takes ~10 seconds to run\n",
    "class_probs = []\n",
    "class_preds = []\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        output = net(images)\n",
    "        class_probs_batch = [F.softmax(el, dim=0) for el in output]\n",
    "        _, class_preds_batch = torch.max(output, 1)\n",
    "\n",
    "        class_probs.append(class_probs_batch)\n",
    "        class_preds.append(class_preds_batch)\n",
    "\n",
    "test_probs = torch.cat([torch.stack(batch) for batch in class_probs])\n",
    "test_preds = torch.cat(class_preds)\n",
    "\n",
    "# helper function\n",
    "def add_pr_curve_tensorboard(class_index, test_probs, test_preds, global_step=0):\n",
    "    '''\n",
    "    Takes in a \"class_index\" from 0 to 9 and plots the corresponding\n",
    "    precision-recall curve\n",
    "    '''\n",
    "    tensorboard_preds = test_preds == class_index\n",
    "    tensorboard_probs = test_probs[:, class_index]\n",
    "\n",
    "    writer.add_pr_curve(classes[class_index],\n",
    "                        tensorboard_preds,\n",
    "                        tensorboard_probs,\n",
    "                        global_step=global_step)\n",
    "    writer.close()\n",
    "\n",
    "# plot all the pr curves\n",
    "for i in range(len(classes)):\n",
    "    add_pr_curve_tensorboard(i, test_probs, test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cu110.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
