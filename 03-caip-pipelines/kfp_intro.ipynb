{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "celltoolbar": "Tags",
    "environment": {
      "name": "tf2-2-3-gpu.2-3.m56",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m56"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    },
    "colab": {
      "name": "Copy of Copy of kfp_intro.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUkyIjOxW7bi"
      },
      "source": [
        "# Managed Pipelines Experimental: KFP SDK introduction\n",
        "\n",
        "This notebook shows an example of building and running a simple pipeline on [Managed Pipelines Experimental](https://docs.google.com/document/d/1JXtowHwppgyghnj1N1CT73hwD1caKtWkLcm2_0qGBoI/edit?ts=5f90dcea#heading=h.p4rp2vtz67w2), using the [Kubeflow Pipelines (KFP) SDK](https://www.kubeflow.org/docs/pipelines/) \n",
        "\n",
        "It shows how to to construct *function-based components* — pipeline components defined from Python function definitions— and how to specify a pipeline using those components, then launch a pipeline run from the notebook.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWACue6PW7bk"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Before you run this notebook, ensure that your Google Cloud user account and project are granted access to the Managed Pipelines Experimental. To be granted access to the Managed Pipelines Experimental, fill out this [form](http://go/cloud-mlpipelines-signup) and let your account representative know you have requested access. \n",
        "\n",
        "This notebook is intended to be run on either one of:\n",
        "* [AI Platform Notebooks](https://cloud.google.com/ai-platform-notebooks). See the \"AI Platform Notebooks\" section in the Experimental [User Guide](https://docs.google.com/document/d/1JXtowHwppgyghnj1N1CT73hwD1caKtWkLcm2_0qGBoI/edit?usp=sharing) for more detail on creating a notebook server instance.\n",
        "* [Google Colab](https://colab.research.google.com/notebooks/intro.ipynb)\n",
        "\n",
        "\n",
        "**To run this notebook on AI Platform Notebooks**, click on the **File** menu, then select \"Download .ipynb\".  Then, upload that notebook from your local machine to AI Platform Notebooks. (In the AI Platform Notebooks left panel, look for an icon of an arrow pointing up, to upload).\n",
        "\n",
        "We'll first install some libraries and set up some variables.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAaCPLjgiJrO"
      },
      "source": [
        "Set `gcloud` to use your project.  **Edit the following cell before running it**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pD5jOcSURdcU"
      },
      "source": [
        "PROJECT_ID = 'your-project-id'  # <---CHANGE THIS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkWdxe4TXRHk"
      },
      "source": [
        "!gcloud config set project {PROJECT_ID}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gckGHdW9iPrq"
      },
      "source": [
        "If you're running this notebook on colab, authenticate with your user account:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZQA0KrfXCvU"
      },
      "source": [
        "import sys\n",
        "if 'google.colab' in sys.modules:\n",
        "  from google.colab import auth\n",
        "  auth.authenticate_user()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaqJjbmk6o0o"
      },
      "source": [
        "-----------------\n",
        "\n",
        "**If you're on AI Platform Notebooks**, authenticate with Google Cloud before running the next section, by running\n",
        "```sh\n",
        "gcloud auth login\n",
        "```\n",
        "**in the Terminal window** (which you can open via **File** > **New** in the menu). You only need to do this once per notebook instance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOpZ41iBW7bl"
      },
      "source": [
        "### Install the KFP SDK and AI Platform Pipelines client library\n",
        "\n",
        "For Managed Pipelines Experimental, you'll need to download a special version of the AI Platform client library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlPnul5UW7bl"
      },
      "source": [
        "!gsutil cp gs://cloud-aiplatform-pipelines/aiplatform_pipelines_client-0.1.0.caip20201109-py3-none-any.whl ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpdfRA4vW7bq"
      },
      "source": [
        "Then, install the libraries and restart the kernel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmUZzSv6YA9-"
      },
      "source": [
        "if 'google.colab' in sys.modules:\n",
        "  USER_FLAG = ''\n",
        "else:\n",
        "  USER_FLAG = '--user'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGdU0lEfVwM-"
      },
      "source": [
        "!python3 -m pip install {USER_FLAG} kfp==1.1.1 --upgrade\n",
        "!python3 -m pip install {USER_FLAG} aiplatform_pipelines_client-0.1.0.caip20201109-py3-none-any.whl --upgrade"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IKZcgZnX3j6"
      },
      "source": [
        "if not 'google.colab' in sys.modules:\n",
        "  # Automatically restart kernel after installs\n",
        "  import IPython\n",
        "  app = IPython.Application.instance()\n",
        "  app.kernel.do_shutdown(True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mqs-ZFuW7bx"
      },
      "source": [
        "The KFP version should be == 1.1.1.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4uvTyimMYOr"
      },
      "source": [
        "# Check the KFP version\n",
        "!python3 -c \"import kfp; print('KFP version: {}'.format(kfp.__version__))\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tskC13YxW7b3"
      },
      "source": [
        "### Set some variables\n",
        "\n",
        "**Before you run the next cell**, **edit it** to set variables for your project.  See the \"Before you begin\" section of the User Guide for information on creating your API key.  For `BUCKET_NAME`, enter the name of a Cloud Storage (GCS) bucket in your project.  Don't include the `gs://` prefix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHsVifdTW7b4"
      },
      "source": [
        "PATH=%env PATH\n",
        "%env PATH={PATH}:/home/jupyter/.local/bin\n",
        "\n",
        "# Required Parameters\n",
        "USER = 'your-user-name' # <---CHANGE THIS\n",
        "BUCKET_NAME = 'your-bucket-name'  # <---CHANGE THIS\n",
        "PIPELINE_ROOT = 'gs://{}/pipeline_root/{}'.format(BUCKET_NAME, USER)\n",
        "\n",
        "PROJECT_ID = 'your-project-id'  # <---CHANGE THIS\n",
        "REGION = 'us-central1'\n",
        "API_KEY = 'your-api-key'  # <---CHANGE THIS\n",
        "\n",
        "print('PIPELINE_ROOT: {}'.format(PIPELINE_ROOT))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbN_49SUW7b7"
      },
      "source": [
        "## Define a simple pipeline\n",
        "\n",
        "Now we'll define a very simple pipeline. It takes one input parameter and has one step.\n",
        "\n",
        "### Create a function-based pipeline component\n",
        "\n",
        "We'll first create a component based on a very simple python function. It takes a string input parameter and returns that value as output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ye7mr8WPW7b8"
      },
      "source": [
        "from kfp.v2 import components\n",
        "\n",
        "def hello_world(text: str):\n",
        "    print(text)\n",
        "    return text\n",
        "\n",
        "components.func_to_container_op(hello_world,\n",
        "      output_component_file='hw.yaml')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84N6V7_jW7b_"
      },
      "source": [
        "Next, we'll define a one-step pipeline that uses that component.\n",
        "The pipeline takes an input parameter, and passes that parameter as an argument to the pipeline step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJkQwWzsW7cA"
      },
      "source": [
        "from kfp.v2 import dsl\n",
        "from kfp.v2 import compiler\n",
        "from kfp.v2 import components\n",
        "\n",
        "\n",
        "# Create a pipeline op from the component we defined above.\n",
        "hw_op = components.load_component_from_file('./hw.yaml') # you can also use load_component_from_url\n",
        "\n",
        "@dsl.pipeline(\n",
        "  name='hello-world',\n",
        "  description='A simple intro pipeline'\n",
        ")\n",
        "def pipeline_parameter_to_consumer(text: str='hi there'):\n",
        "    '''Pipeline that passes small pipeline parameter string to consumer op'''\n",
        "    consume_task = hw_op(text) # Passing pipeline parameter as argument to consumer op\n",
        "    \n",
        "pipeline_func = pipeline_parameter_to_consumer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foOQ6jYYR3Cp"
      },
      "source": [
        "Compile the pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qf9KkkoA1y7"
      },
      "source": [
        "compiler.Compiler().compile(pipeline_func=pipeline_func, \n",
        "                            pipeline_root=PIPELINE_ROOT,\n",
        "                            output_path='hw_pipeline_job.json')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mH5QFfSuW7cJ"
      },
      "source": [
        "## Submit the pipeline job\n",
        "\n",
        "Here, we'll create an API client using the API key you generated.\n",
        "\n",
        "Then, we'll submit the pipeline job by passing the compiled spec to the `create_run_from_job_spec()` method. Note that we're passing a `parameter_values` dict that specifies the pipeline input parameters we want to use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSnrYUDAW7cK"
      },
      "source": [
        "from aiplatform.pipelines import client\n",
        "\n",
        "api_client = client.Client(project_id=PROJECT_ID, region=REGION, api_key=API_KEY)\n",
        "\n",
        "response = api_client.create_run_from_job_spec(\n",
        "          job_spec_path='hw_pipeline_job.json',\n",
        "          # pipeline_root=PIPELINE_ROOT,  # optional- use if want to override compile-time value\n",
        "          parameter_values={'text': 'Hello world!'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Kgtx8-bW7cM"
      },
      "source": [
        "### Monitor the pipeline run in the Cloud Console\n",
        "\n",
        "Once you've deployed the pipeline run, you can monitor it in the [Cloud Console](https://console.cloud.google.com/ai/platform/pipelines) under **AI Platform (Unified)** > **Pipelines**. \n",
        "\n",
        "Click in to the pipeline run to see the run graph (for our simple pipeline, this consists of one step), and click on a step to view the job detail and the logs for that step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b-WfSe-5sd4"
      },
      "source": [
        "### Submit a pipeline job via the Cloud Console\n",
        "\n",
        "You can also submit pipeline jobs directly through the Cloud Console.  To see this, download the `hw_pipeline_job.json` file generated by the compilation.\n",
        "\n",
        "In the Console, click **CREATE RUN**.  \n",
        "\n",
        "<a href=\"https://storage.googleapis.com/amy-jo/images/kf-pls/create_run.png\" target=\"_blank\"><img src=\"https://storage.googleapis.com/amy-jo/images/kf-pls/create_run.png\" width=\"40%\"/></a>\n",
        "\n",
        "Upload `hw_pipeline_job.json` and give your run a name (this must be unique).  \n",
        "\n",
        "<a href=\"https://storage.googleapis.com/amy-jo/images/kf-pls/run_details.png\" target=\"_blank\"><img src=\"https://storage.googleapis.com/amy-jo/images/kf-pls/run_details.png\" width=\"50%\"/></a>\n",
        "\n",
        "You can enter a input parameter value if you like.\n",
        "\n",
        "<a href=\"https://storage.googleapis.com/amy-jo/images/kf-pls/run_parameters.png\" target=\"_blank\"><img src=\"https://storage.googleapis.com/amy-jo/images/kf-pls/run_parameters.png\" width=\"50%\"/></a>\n",
        "\n",
        "Then click **SUBMIT**.  You'll see your new pipeline run start up in the Console."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feV62LXyW7cN"
      },
      "source": [
        "## What next?\n",
        "\n",
        "Next, try out notebooks with more complex pipelines.  These pipelines show how to pass data between components and build more complex components.\n",
        "\n",
        "- a simple KFP example that [shows how data can be passed between pipeline steps](https://colab.research.google.com/drive/1NztsGV-FAp71MU7zfMHU0SlfQ8dpw-9u).\n",
        "- a KFP example that [shows building custom components for data processing and training](https://colab.research.google.com/drive/1CV5SgrhRp0bgJcFKGc0G5oWwGTHc7bqt?usp=sharing). It also shows how to pass typed artifact data between components,  how to specify required resources when defining a pipeline, and how to query a pipeline's metadata.\n",
        "- A TFX notebook that [shows the canonical 'Chicago taxi' example](https://colab.research.google.com/drive/1dNLlm21F6f5_4aeIg-Zs_F1iGGRPEvhW), and how to use custom Python functions and custom containers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89fYarRLW7cN"
      },
      "source": [
        "-----------------------------\n",
        "Copyright 2020 Google LLC\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "     http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License."
      ]
    }
  ]
}