{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "celltoolbar": "Tags",
    "environment": {
      "name": "tf2-2-3-gpu.2-3.m56",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m56"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    },
    "colab": {
      "name": "mp_kfp_producer_consumer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRK6QyOelbDH"
      },
      "source": [
        "# KFP SDK: Component I/O and passing data between pipeline components\n",
        "\n",
        "\n",
        "\n",
        "In this notebook, we'll build an example that shows how data can be passed between pipeline steps.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWACue6PW7bk"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Before you run this notebook, ensure that your Google Cloud user account and project are granted access to the Managed Pipelines Experimental. To be granted access to the Managed Pipelines Experimental, fill out this [form](http://go/cloud-mlpipelines-signup) and let your account representative know you have requested access. \n",
        "\n",
        "This notebook is intended to be run on either one of:\n",
        "* [AI Platform Notebooks](https://cloud.google.com/ai-platform-notebooks). See the \"AI Platform Notebooks\" section in the Experimental [User Guide](https://docs.google.com/document/d/1JXtowHwppgyghnj1N1CT73hwD1caKtWkLcm2_0qGBoI/edit?usp=sharing) for more detail on creating a notebook server instance.\n",
        "* [Google Colab](https://colab.research.google.com/notebooks/intro.ipynb)\n",
        "\n",
        "**To run this notebook on AI Platform Notebooks**, click on the **File** menu, then select \"Download .ipynb\".  Then, upload that notebook from your local machine to AI Platform Notebooks. (In the AI Platform Notebooks left panel, look for an icon of an arrow pointing up, to upload).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAaCPLjgiJrO"
      },
      "source": [
        "We'll first install some libraries and set up some variables.\n",
        "\n",
        "Set `gcloud` to use your project.  **Edit the following cell before running it**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pD5jOcSURdcU"
      },
      "source": [
        "PROJECT_ID = 'your-project-id'  # <---CHANGE THIS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkWdxe4TXRHk"
      },
      "source": [
        "!gcloud config set project {PROJECT_ID}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HP2mWoHmUt3"
      },
      "source": [
        "If you're running this notebook on colab, authenticate with your user account:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxO4CxWZmVrX"
      },
      "source": [
        "import sys\n",
        "if 'google.colab' in sys.modules:\n",
        "  from google.colab import auth\n",
        "  auth.authenticate_user()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaqJjbmk6o0o"
      },
      "source": [
        "-----------------\n",
        "\n",
        "**If you're on AI Platform Notebooks**, authenticate with Google Cloud before running the next section, by running\n",
        "```sh\n",
        "gcloud auth login\n",
        "```\n",
        "**in the Terminal window** (which you can open via **File** > **New** in the menu). You only need to do this once per notebook instance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-H9tO_fzlbDO"
      },
      "source": [
        "### Install the KFP SDK and AI Platform Pipelines client library\n",
        "\n",
        "For Managed Pipelines Experimental, you'll need to download a special version of the AI Platform client library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlPnul5UW7bl"
      },
      "source": [
        "!gsutil cp gs://cloud-aiplatform-pipelines/aiplatform_pipelines_client-0.1.0.caip20201109-py3-none-any.whl ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "injJzlmllbEL"
      },
      "source": [
        "Then, install the libraries and restart the kernel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmUZzSv6YA9-"
      },
      "source": [
        "if 'google.colab' in sys.modules:\n",
        "  USER_FLAG = ''\n",
        "else:\n",
        "  USER_FLAG = '--user'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_RkBATOW7bq"
      },
      "source": [
        "!python3 -m pip install {USER_FLAG} kfp==1.1.1 --upgrade\n",
        "!python3 -m pip install {USER_FLAG} aiplatform_pipelines_client-0.1.0.caip20201109-py3-none-any.whl --upgrade"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5kaReN2lbEN"
      },
      "source": [
        "if not 'google.colab' in sys.modules:\n",
        "  # Automatically restart kernel after installs\n",
        "  import IPython\n",
        "  app = IPython.Application.instance()\n",
        "  app.kernel.do_shutdown(True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N33S1ikHIOPS"
      },
      "source": [
        "The KFP version should be == 1.1.1.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4uvTyimMYOr"
      },
      "source": [
        "# Check the KFP version\n",
        "!python3 -c \"import kfp; print('KFP version: {}'.format(kfp.__version__))\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tskC13YxW7b3"
      },
      "source": [
        "### Set some variables\n",
        "\n",
        "**Before you run the next cell**, **edit it** to set variables for your project.  See the \"Before you begin\" section of the User Guide for information on creating your API key.  For `BUCKET_NAME`, enter the name of a Cloud Storage (GCS) bucket in your project.  Don't include the `gs://` prefix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlWHwdmTlbFO"
      },
      "source": [
        "PATH=%env PATH\n",
        "%env PATH={PATH}:/home/jupyter/.local/bin\n",
        "\n",
        "# Required Parameters\n",
        "USER = 'your-user-name' # <---CHANGE THIS\n",
        "BUCKET_NAME = 'your-bucket-name'  # <---CHANGE THIS\n",
        "PIPELINE_ROOT = 'gs://{}/pipeline_root/{}'.format(BUCKET_NAME, USER)\n",
        "\n",
        "PROJECT_ID = 'your-project-id'  # <---CHANGE THIS\n",
        "REGION = 'us-central1'\n",
        "API_KEY = 'your-api-key'  # <---CHANGE THIS\n",
        "\n",
        "print('PIPELINE_ROOT: {}'.format(PIPELINE_ROOT))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DleDtvJNlbFk"
      },
      "source": [
        "## Simple two-step pipeline with 'producer' and 'consumer' steps\n",
        "\n",
        "Now we're ready to define and run a pipeline.  We'll build an example that shows how data can be passed between pipeline steps.\n",
        "\n",
        "\n",
        "We'll first do some imports:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLxc88S-lbFq"
      },
      "source": [
        "import time\n",
        "from kfp.v2 import components\n",
        "from kfp.v2 import compiler\n",
        "from kfp.v2 import dsl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYuWor7aqg2r"
      },
      "source": [
        "Next, we'll define the pipeline components.  We'll do this via the `load_component_from_text` method, which expects a string in `yaml` syntax. \n",
        "\n",
        "Both components use as their base container image the `google/cloud-sdk` image, and run a series of shell commands.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHScqAqvv50c"
      },
      "source": [
        "### Producer component\n",
        "\n",
        "The **Producer** component takes as inputs an `inputValue` parameter (`input_text`, of type `String`), and as output an `outputPath` parameter (`output_value`) and an `outputURI` parameter (`output_artifact`).\n",
        "\n",
        "When the op runs, a Cloud Storage (GCS) path will be generated automatically for `output_value`, which for this component is the second argument passed to the shell command (`$1`). Similarly, a GCS path will be generated for the `output_artifact` (`$2`).\n",
        "\n",
        "When the component op runs, the input values are echoed, and then the `input_text` value is copied via `gsutil` to those paths, making those outputs available for consumption by other components. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzBa9Wm3lbGN"
      },
      "source": [
        "producer_op = components.load_component_from_text(\"\"\"\n",
        "name: Producer\n",
        "inputs:\n",
        "- {name: input_text, type: String, description: 'Represents an input parameter.'}\n",
        "outputs:\n",
        "- {name: output_value, type: String, description: 'Represents an output paramter.'}\n",
        "- {name: output_artifact, description: 'Represents an output artifact.'}\n",
        "implementation:\n",
        "  container:\n",
        "    image: google/cloud-sdk:latest\n",
        "    command:\n",
        "    - sh\n",
        "    - -c\n",
        "    - |\n",
        "      set -e -x\n",
        "      echo \"$0, this is an output parameter\" | gsutil cp - \"$1\"\n",
        "      echo \"$0, this is an output artifact\" | gsutil cp - \"$2\"\n",
        "    - {inputValue: input_text}\n",
        "    - {outputPath: output_value}\n",
        "    - {outputUri: output_artifact}\n",
        "\"\"\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWW5ryA6snzA"
      },
      "source": [
        "### Consumer component\n",
        "\n",
        "The **Consumer** component has two inputs. When the component op runs, the input values are echoed.  We'll define our pipeline so that the `input_value` and `input_artifact` of the Consumer step are obtained from the outputs of the **Producer** step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASo_EZ-RlbGU"
      },
      "source": [
        "consumer_op = components.load_component_from_text(\"\"\"\n",
        "name: Consumer\n",
        "inputs:\n",
        "- {name: input_value, type: String, description: 'Represents an input parameter. It connects to an upstream output parameter.'}\n",
        "- {name: input_artifact, description: 'Represents an input artifact. It connects to an upstream output artifact.'}\n",
        "implementation:\n",
        "  container:\n",
        "    image: google/cloud-sdk:latest\n",
        "    command:\n",
        "    - sh\n",
        "    - -c\n",
        "    - |\n",
        "      set -e -x\n",
        "      echo \"Read from an input parameter: \" && echo \"$0\"\n",
        "      echo \"Read from an input artifact: \" && gsutil cat \"$1\"\n",
        "    - {inputValue: input_value}\n",
        "    - {inputUri: input_artifact}\n",
        "\"\"\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZosf60CpqYP"
      },
      "source": [
        "## Define a pipeline using the components and submit a run\n",
        "\n",
        "Next, we'll define a two-step pipeline that uses the producer and consumer ops.  We're setting the Consumer op's `input_value` arg to the output of the Producer op; specifically, the `output_value` output.  Under the hood, this value is obtained by automatically reading the GCS file path to which the Producer step wrote."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qa7JRpknlbGX"
      },
      "source": [
        "@dsl.pipeline(name='simple-two-step-pipeline-{}-{}'.format(USER, str(int(time.time()))))\n",
        "def two_step_pipeline(\n",
        "    text = 'Hello world'\n",
        "):\n",
        "  producer = producer_op(input_text=text)\n",
        "  consumer = consumer_op(input_value=producer.outputs['output_value'], \n",
        "                         input_artifact=producer.outputs['output_artifact'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJL7COO1qIWJ"
      },
      "source": [
        "Compile the pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qf9KkkoA1y7"
      },
      "source": [
        "compiler.Compiler().compile(pipeline_func=two_step_pipeline, \n",
        "                            pipeline_root=PIPELINE_ROOT,\n",
        "                            output_path='two_step_pipeline_job.json')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mH5QFfSuW7cJ"
      },
      "source": [
        "### Submit the pipeline job\n",
        "\n",
        "Here, we'll create an API client using the API key you generated.\n",
        "\n",
        "Then, we'll submit the pipeline job by passing the compiled spec to the `create_run_from_job_spec()` method. Note that we're passing a `parameter_values` dict that specifies the pipeline input parameters we want to use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSnrYUDAW7cK"
      },
      "source": [
        "from aiplatform.pipelines import client\n",
        "\n",
        "api_client = client.Client(project_id=PROJECT_ID, region=REGION, api_key=API_KEY)\n",
        "\n",
        "response = api_client.create_run_from_job_spec(\n",
        "    job_spec_path='two_step_pipeline_job.json',\n",
        "    # pipeline_root=PIPELINE_ROOT,  # optional- use if want to override compile-time value\n",
        "    parameter_values={'text': 'This is some input text'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Kgtx8-bW7cM"
      },
      "source": [
        "### Monitor the pipeline run in the Cloud Console\n",
        "\n",
        "Once you've deployed the pipeline run, you can monitor it in the [Cloud Console](https://console.cloud.google.com/ai/platform/pipelines) under **AI Platform (Unified)** > **Pipelines**. \n",
        "\n",
        "Click in to the pipeline run to see the run graph (for our simple pipeline, this consists of two steps), and click on a step to view the job detail and the logs for that step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbJqsTfv4Lbz"
      },
      "source": [
        "<!-- <a href=\"https://storage.googleapis.com/amy-jo/images/kf-pls/producer_consumer.png\" target=\"_blank\"><img src=\"https://storage.googleapis.com/amy-jo/images/kf-pls/producer_consumer.png\" width=\"50%\"/></a> -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feV62LXyW7cN"
      },
      "source": [
        "## What next?\n",
        "\n",
        "Next, try out some of the other notebooks.\n",
        "\n",
        "- a [KFP intro notebook](https://colab.research.google.com/drive/1mrud9HjsVp5fToHwwNL0RotFtJCKtfZ1#scrollTo=feV62LXyW7cN).\n",
        "- a KFP example that [shows building custom components for data processing and training](https://colab.research.google.com/drive/1CV5SgrhRp0bgJcFKGc0G5oWwGTHc7bqt?usp=sharing). It also shows how to pass typed artifact data between component, and how to specify required resources when defining a pipeline.\n",
        "\n",
        "- A TFX notebook that [shows the canonical 'Chicago taxi' example](https://colab.research.google.com/drive/1dNLlm21F6f5_4aeIg-Zs_F1iGGRPEvhW), and how to use custom Python functions and custom containers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMqhSncJlbHU"
      },
      "source": [
        "-----------------------------\n",
        "Copyright 2020 Google LLC\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "     http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License."
      ]
    }
  ]
}